{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Performance Testing<br>\n",
    "Results from the model_selection_v0 python notebook got me here, which is testing some optimized Logistic Regression models for use in predicting risk of 7 day mortality. In this notebook, I'll be finalizing the models and reporting on performance of the trained models on validation data, using 10-fold cross validation. <br><br>\n",
    "\n",
    "Additionally, other models will be ran (kNN, decision tree, maybe neural net) to show other future options, show a performance ceiling given the data inputs, despite the lack of model interpretability. I think a long-term modeling option is deep learning methods, or even XGboost, but right now, the models need to be fully interpretable. Thus, the focus on logistic regression.<br><br>\n",
    "\n",
    "While the previous notebook identified median columnar imputation of missing data as the superior method (when compared to adding 0 for missing data), I'm still going to test each model version against both methods of imputation, as I think it would be worthwhile to quantify the improvements in model performance with median imputation. Additionally, seeing errors for each imputation method will be informative. <br><br>\n",
    "\n",
    "One data processing step I would like to also test is z-scaling (normalizing data to mean = 0, sd = 1) for all data. By doing this, I think I will get a better representation of which variables appear to be most informative of the outcome, which again will help with interpretability clinically. While yes, the scales themselves will need to be re-converted for direct interpretation, I don't think this will be necessary, as most people right now just want to know WHAT is informative, not necessarily how much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, Imputer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFECV\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "from IPython.display import display #displays full dataframe columns\n",
    "#display all dataframe columns when printed\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271541, 120)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "df = pd.read_csv('C:/Users/Mark.Burghart/Documents/projects/hospice_carepoint/data/transformed/carepoint_transformed_dummied.csv', index_col=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split<br>\n",
    "Same split and randomizer will be used from the previous model_selection_v0 notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271541, 119)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#separate variables (X) from outcome of interest (y)\n",
    "df.shape\n",
    "cols = df.columns.get_values() #converts column names to list\n",
    "cols = cols.tolist()\n",
    "feature_cols = [x for x in cols if x != 'death_within_7_days'] #removes outcome of interest from list ('death_within_7_days')\n",
    "\n",
    "#extract rows\n",
    "#print(feature_cols) #debug\n",
    "X = df.loc[:, feature_cols]\n",
    "X.shape #outcome column has been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271541,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save outcome variable as y\n",
    "y = df.death_within_7_days\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate data into training/test (aka holdout) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 23) #random_state for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create objects with specific imputation method\n",
    "#impute missing values: replacing NaNs with Median Column value for each column\n",
    "X_train_med = X_train.fillna(X_train.median()) \n",
    "y_train_med = y_train.fillna(y_train.median())\n",
    "\n",
    "#impute missing values: replacing NaNs with 0\n",
    "X_train_zero = X_train.fillna(0)\n",
    "y_train_zero = y_train.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## Model Training<br>\n",
    "\n",
    "Below, I have created a python sklearn pipeline to implement a series of steps before acquiring outputs. First, the data is standardized to a mean of 0 and a SD of 1 (aka. Z-scaling, standard scaling). Next, the top 40 features are selected based on F-test of ANOVA procedure. Finally, a logistic regression (L1 or L2) is built with specified C parameter obtained from v0 model selection notebook. Lastly, the scores for each cross validation fold are printed, along with the collective cross validation mean from the 10 folds. <br><br>\n",
    "#### L1 LASSO Logistic Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#pipeline for cross validation, preprocessing, feature selection, and model training of L1 Logistic Regression\n",
    "L1_pipeline_scaled = make_pipeline(StandardScaler(), SelectKBest(f_classif, k = 40), LogisticRegression(penalty = 'l1', random_state = 0, C = 100))\n",
    "\n",
    "#L1 median imputation data\n",
    "scores_L1_med_scaled = cross_val_score(L1_pipeline_scaled, X_train_med, y_train_med, cv = 10, scoring = 'roc_auc')\n",
    "\n",
    "#L1 zero imputation data\n",
    "scores_L1_zero_scaled = cross_val_score(L1_pipeline_scaled, X_train_zero, y_train_zero, cv = 10, scoring = 'roc_auc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation AUC scores from each 10-fold run for L1 Regression (Median impute, scaled): \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.80269777, 0.81250126, 0.80551748, 0.81138236, 0.80665629,\n",
       "       0.80834911, 0.8013815 , 0.81326675, 0.80625669, 0.80557533])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#L1 median imputation data, scaled data\n",
    "print(\"Cross validation AUC scores from each 10-fold run for L1 Regression (Median impute, scaled): \")\n",
    "scores_L1_med_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross validation AUC from 10-fold CV for L1 Regression (Median impute, scaled): \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8073584550683715"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mean cross validation AUC from 10-fold CV for L1 Regression (Median impute, scaled): \")\n",
    "scores_L1_med_scaled.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Average AUC from the 10-fold cross validation procedure was 80.7%, ranging from 80.1 to 81.3% for the 10 folds. This is pretty great, and only includes the top 40 features from the input data. Again, the above model used the median column value for missing data. Additionally, this data has been scaled..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation AUC scores from each 10-fold run for L1 Regression (Zero impute, scaled): \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.75179977, 0.76627586, 0.75610132, 0.75909766, 0.75957083,\n",
       "       0.76302297, 0.75603777, 0.76004062, 0.75710745, 0.75490602])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#L1 zero imputation data, scaled data\n",
    "print(\"Cross validation AUC scores from each 10-fold run for L1 Regression (Zero impute, scaled): \")\n",
    "scores_L1_zero_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross validation AUC from 10-fold CV for L1 Regression (Zero impute, scaled): \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7583960260662035"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mean cross validation AUC from 10-fold CV for L1 Regression (Zero impute, scaled): \")\n",
    "scores_L1_zero_scaled.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Average AUC from the same 10-fold cross validation was 75.8%, ranging from 75.2 - 76.6% for each fold. The main difference here is the median wasn't calculated for the missing values, and instead '0' replaced the NaNs. There is a clear performance benefit of 5% increase in AUC when median values were calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#pipeline for cross validation, preprocessing, feature selection, and model training of L1 Logistic Regression\n",
    "L1_pipeline = make_pipeline(SelectKBest(f_classif, k = 40), LogisticRegression(penalty = 'l1', random_state = 0, C = 100))\n",
    "\n",
    "#L1 median imputation data\n",
    "scores_L1_med = cross_val_score(L1_pipeline, X_train_med, y_train_med, cv = 10, scoring = 'roc_auc')\n",
    "\n",
    "#L1 zero imputation data\n",
    "scores_L1_zero = cross_val_score(L1_pipeline, X_train_zero, y_train_zero, cv = 10, scoring = 'roc_auc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation AUC scores from each 10-fold run for L1 Regression (Median impute): \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.80269062, 0.81250074, 0.80551143, 0.81136767, 0.80665393,\n",
       "       0.80837599, 0.80135174, 0.81326586, 0.80625368, 0.80557547])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#L1 median imputation data\n",
    "print(\"Cross validation AUC scores from each 10-fold run for L1 Regression (Median impute): \")\n",
    "scores_L1_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross validation AUC from 10-fold CV for L1 Regression (Median impute): \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8073547133321363"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mean cross validation AUC from 10-fold CV for L1 Regression (Median impute): \")\n",
    "scores_L1_med.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation AUC scores from each 10-fold run for L1 Regression (Zero impute): \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.75179663, 0.76627235, 0.75610286, 0.75911345, 0.75958953,\n",
       "       0.76302976, 0.75604153, 0.76004666, 0.75710188, 0.75491917])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#L1 zero imputation data\n",
    "print(\"Cross validation AUC scores from each 10-fold run for L1 Regression (Zero impute): \")\n",
    "scores_L1_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross validation AUC from 10-fold CV for L1 Regression (Zero impute): \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.75840138199799"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mean cross validation AUC from 10-fold CV for L1 Regression (Zero impute): \")\n",
    "scores_L1_zero.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 Ridge Penalty Logistic Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#pipeline for CV, feature selection, model training of L2 Logistic Regression\n",
    "L2_pipeline_scaled = make_pipeline(StandardScaler(), SelectKBest(f_classif, k = 40), LogisticRegression(penalty = 'l2', random_state = 10, C = 10)) #different randomizer from L1\n",
    "\n",
    "#L2 median imputation data, scaled\n",
    "scores_L2_med_scaled = cross_val_score(L2_pipeline_scaled, X_train_med, y_train_med, cv = 10, scoring = 'roc_auc')\n",
    "\n",
    "#L2 zero imputation data, scaled\n",
    "scores_L2_zero_scaled = cross_val_score(L2_pipeline_scaled, X_train_zero, y_train_zero, cv = 10, scoring = 'roc_auc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation AUC scores from each 10-fold run for L2 Regression (Median impute,scaled): \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.80269884, 0.8125002 , 0.80551822, 0.81138461, 0.80665888,\n",
       "       0.80834465, 0.80138623, 0.81326632, 0.80625643, 0.80557416])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Cross validation AUC scores from each 10-fold run for L2 Regression (Median impute,scaled): \")\n",
    "scores_L2_med_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross validation AUC from 10-fold CV for L2 Regression (Median impute, scaled): \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8073588531679465"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mean cross validation AUC from 10-fold CV for L2 Regression (Median impute, scaled): \")\n",
    "scores_L2_med_scaled.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different randomizer was used for the L2 CV testing. Despite this, the same CV AUC average was produced when compared to L1 regression model with Median imputation. Which is crazy... Believable, as these methods produce similarly coefficients, but weird..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation AUC scores from each 10-fold run for L2 Regression (Zero impute, scaled): \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.75179925, 0.76627598, 0.75610328, 0.75910407, 0.75957499,\n",
       "       0.76301923, 0.7560398 , 0.76003704, 0.75710971, 0.75490946])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Cross validation AUC scores from each 10-fold run for L2 Regression (Zero impute, scaled): \")\n",
    "scores_L2_zero_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross validation AUC from 10-fold CV for L2 Regression (Zero impute, scaled): \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7583972815173426"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mean cross validation AUC from 10-fold CV for L2 Regression (Zero impute, scaled): \")\n",
    "scores_L2_zero_scaled.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#pipeline for CV, feature selection, model training of L2 Logistic Regression NON SCALED DATA\n",
    "L2_pipeline = make_pipeline(SelectKBest(f_classif, k = 40), LogisticRegression(penalty = 'l2', random_state = 10, C = 10)) #different randomizer from L1\n",
    "\n",
    "#L2 median imputation data\n",
    "scores_L2_med = cross_val_score(L2_pipeline, X_train_med, y_train_med, cv = 10, scoring = 'roc_auc')\n",
    "\n",
    "#L2 zero imputation data\n",
    "scores_L2_zero = cross_val_score(L2_pipeline, X_train_zero, y_train_zero, cv = 10, scoring = 'roc_auc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation AUC scores from each 10-fold run for L2 Regression (Median impute): \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.80270102, 0.81250332, 0.80551896, 0.8113836 , 0.80666229,\n",
       "       0.8083408 , 0.8013774 , 0.81326762, 0.80626859, 0.80556882])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Cross validation AUC scores from each 10-fold run for L2 Regression (Median impute): \")\n",
    "scores_L2_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross validation AUC from 10-fold CV for L2 Regression (Median impute): \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8073592406338486"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mean cross validation AUC from 10-fold CV for L2 Regression (Median impute): \")\n",
    "scores_L2_med.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation AUC scores from each 10-fold run for L2 Regression (Zero impute): \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.75178656, 0.76629526, 0.75610754, 0.75909635, 0.75958227,\n",
       "       0.76302645, 0.75604576, 0.76004014, 0.75712032, 0.75491117])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Cross validation AUC scores from each 10-fold run for L2 Regression (Zero impute): \")\n",
    "scores_L2_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross validation AUC from 10-fold CV for L2 Regression (Zero impute): \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7584011824166922"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Mean cross validation AUC from 10-fold CV for L2 Regression (Zero impute): \")\n",
    "scores_L2_zero.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "<BR> L1 vs L2 regressions did not yield much separation. Neither did scaling of the data as a preprocessing step. Moving forward, I'll eliminate the scaling to improve feature interpretation. Lastly, median imputation of missing data is important, and if possible, should be considered for production. A difference of 5% is significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regression model and performance<br>\n",
    "I am pursuing a logistic regression with L1 penalty to reduce trouble with interpretation. L1 penalty sets coefficients to 0 if the variable appears to have little influence in the outcome prediction. L2 does not. Setting the coefficient to 0 basically removes the variable from the model, making the interpretation easier.\n",
    "<br><br>\n",
    "#### L1 with '0' imputed for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "\n",
    "#pipeline for cross validation, preprocessing, feature selection, and model training of L1 Logistic Regression\n",
    "L1_pipeline = make_pipeline(SelectKBest(f_classif, k = 40), LogisticRegression(penalty = 'l1', random_state = 30, C = 100))\n",
    "\n",
    "y_pred = cross_val_predict(L1_pipeline, X_train_zero, y_train_zero, cv = 10)\n",
    "conf_matrix_zero = confusion_matrix(y_train_zero, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "results = cross_val_score(L1_pipeline, X_train_zero, y_train_zero, cv = 10, scoring = \"roc_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7584034276525773"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above AUC represents the model's ability to discriminate between positive (going to die within 7 days) or negative (not going to). 1.0 would be a perfect model. 0.5 would be a random model (or flipping a coin). We want to be better than 0.5, and as close to 1 as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[138543,   6264],\n",
       "       [ 30788,  14483]], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = conf_matrix_zero[1, 1]\n",
    "TN = conf_matrix_zero[0, 0]\n",
    "FP = conf_matrix_zero[0, 1]\n",
    "FN = conf_matrix_zero[1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8050694977851198"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#classification accuracy: how often the classifier is correct\n",
    "# use float to perform true division, not integer division\n",
    "class_acc = ((TP + TN) / float(TP + TN + FP + FN))\n",
    "class_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3199178281902322\n"
     ]
    }
   ],
   "source": [
    "#sensitivity: When the visit resulted in death within 7 days, how often is the prediction correct?\n",
    "#want to maximize\n",
    "# aka Recall\n",
    "sens = TP / float(FN + TP)\n",
    "print(sens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity is pretty low. 32% is not ideal, and is heavily influenced by the number of False Negatives missed by the algorithm. 31,000 instances of the algorithm said the visit didn't occur within 7 days of death and the person actually passed away within 7 days. <br><br>\n",
    "\n",
    "To help resolve some of this, I could decrease the decision threshold for the prediction. For instance, instead of any value > 0.5 probability of the event, I could change this to >0.3 and the instance would be classified as \"yes, death within 7 days\". Doing this will increase the number of false positives and decrease the PPV, but ultimately, I think we need to improve and reduce the number of False Negatives. Phrasing the performance another way, the model is only catching ~32% of the positive events (sensitivity). This is a good starting point, but again, I think we can improve this!\n",
    "\n",
    "ref: http://www.ritchieng.com/machine-learning-evaluate-classification-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04325757732706292"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#false positive rate: when actual value is negative (no death in 7 days), how often is prediction wrong?\n",
    "FPR = FP / float(TN + FP)\n",
    "FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6980768303851159"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Precision, also known as Positive Predictive Value (PPV), when positive value predicted (event will occur), how often is this prediction correct?\n",
    "precision = TP / float(TP + FP)\n",
    "precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.96      0.88    144807\n",
      "          1       0.70      0.32      0.44     45271\n",
      "\n",
      "avg / total       0.79      0.81      0.78    190078\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_train_zero, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary Performance Metrics of 'zero' imputation data on CV Training**\n",
    "<br><br>\n",
    "Sensitivity was pretty low and I think can be improved a bit, even with sacrificing a rise in the False Positive Rate. Currently, the algorithm has a FPR of 5%, which is really low (which is great), but again not in the instance of sacrificing sensitivity. Precision (PPV) was also really good at ~70%. I'd like to keep this here, but will likely slip a bit with any improvements in sensitivity. The AUC of this model version is again 75.8%, which is great. Additionally, I'm also looking at maximizing the F1-score, as this is a weighted average between sensitivity and PPV, with 1 being great and 0 being shitty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 with Median imputed for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "feat_sel = SelectKBest(f_classif, k=40)  \n",
    "clf = LogisticRegression(penalty = 'l1', random_state = 30, C = 100, class_weight='balanced')\n",
    "\n",
    "L1_pipeline = Pipeline([('k_best', feat_sel),\n",
    "                 ('lr', clf)])\n",
    "\n",
    "# Now fit the pipeline using on median imputation data\n",
    "L1_pipeline.fit(X_train_med, y_train_med)\n",
    "\n",
    "y_pred = cross_val_predict(L1_pipeline, X_train_med, y_train_med, cv = 10)\n",
    "conf_matrix_med = confusion_matrix(y_train_med, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(L1_pipeline, X_train_med, y_train_med, cv = 10, scoring = \"roc_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC = 80.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix_med"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False negatives did decrease with the median calculations, but the false positive also increased slightly. Not a cause for concern, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = conf_matrix_med[1, 1]\n",
    "TN = conf_matrix_med[0, 0]\n",
    "FP = conf_matrix_med[0, 1]\n",
    "FN = conf_matrix_med[1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification accuracy: how often the classifier is correct\n",
    "# use float to perform true division, not integer division\n",
    "class_acc = ((TP + TN) / float(TP + TN + FP + FN))\n",
    "class_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correctly classified 81.5% of visits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sensitivity: When the visit resulted in death within 7 days, how often is the prediction correct?\n",
    "#want to maximize\n",
    "# aka Recall\n",
    "sens = TP / float(FN + TP)\n",
    "print(sens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity improved with median imputation, but still pretty low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#false positive rate: when actual value is negative (no death in 7 days), how often is prediction wrong?\n",
    "FPR = FP / float(TN + FP)\n",
    "FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision, also known as Positive Predictive Value (PPV), when positive value predicted (event will occur), how often is this prediction correct?\n",
    "precision = TP / float(TP + FP)\n",
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great PPV. Right at 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_train_med, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1-score improves for identifying positive events, from 0.44 to 0.51. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next steps before final testing**<br><br>\n",
    "start adjusting the threshold to see if improving sensitivity increases AUC, or if we can keep AUC constant while minimizing the False Negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print first 10 predicted responses in training set\n",
    "L1_pipeline.predict(X_train_med)[0:10]\n",
    "\n",
    "L1_pipeline.predict_proba(X_train_med)[0:10]\n",
    "#store predicted probabilities\n",
    "y_pred_prob = L1_pipeline.predict_proba(X_train_med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram of predicted probabilities\n",
    "plt.rcParams['font.size'] = 12\n",
    "#8 bins\n",
    "plt.hist(y_pred_prob, bins = 8)\n",
    "#set limits\n",
    "plt.xlim(0,1)\n",
    "plt.title('Histogram of predicted probabilities')\n",
    "plt.xlabel('Predicted probability of 7 day mortality (trainingset)')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, Orange represents the probability of death within 7 days. Blue show inverse. Notably, most of the probabilities are on the lower end, which sets the case for adjusting the decision point threshold for the binary classifier. Few observations are above 0.5. Adjusting the threshold from 0.5 to something lower will help with the sensitivity and decreasing the False Negatives (while conversely increasing false positives). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold adjustments to improve sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "y_pred_class = binarize(y_pred_prob, 0.35) #returns 1 for all values above .35 and 0 otherwise\n",
    "y_pred_class = y_pred_class[:, 1]\n",
    "y_pred_prob[0:10]\n",
    "y_pred_class[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new confusion matrix\n",
    "print(metrics.confusion_matrix(y_train_med, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old confusion matrix\n",
    "conf_matrix_med"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False positives increased from 7,700 to 18,500. False negatives dropped a 7 thousand... Probably too significant of a drop in threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = binarize(y_pred_prob, 0.42)\n",
    "y_pred_class = y_pred_class[:, 1]\n",
    "print(metrics.confusion_matrix(y_train_med, y_pred_class))\n",
    "y_pred_prob_cut = y_pred_prob[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks to be a better middle ground. FN drops ~ 4,000, while FP increases ~ 5,000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new performance statistics\n",
    "new_conf_matrix = metrics.confusion_matrix(y_train_med, y_pred_class)\n",
    "TP = new_conf_matrix[1, 1]\n",
    "TN = new_conf_matrix[0, 0]\n",
    "FP = new_conf_matrix[0, 1]\n",
    "FN = new_conf_matrix[1, 0]\n",
    "class_acc = ((TP + TN) / float(TP + TN + FP + FN))\n",
    "class_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sensitivity\n",
    "sens = TP / float(FN + TP)\n",
    "print(sens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity jumps from 39.6% with median L1 .50 cutoff, to 47.9% with median L1 @ 0.42 cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#false positive rate: when actual value is negative (no death in 7 days), how often is prediction wrong?\n",
    "FPR = FP / float(TN + FP)\n",
    "FPR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FPR increases from 5% to 8.5%, which I think is still acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision, also known as Positive Predictive Value (PPV), when positive value predicted (event will occur), how often is this prediction correct?\n",
    "precision = TP / float(TP + FP)\n",
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPV drops from ~70% to 64%. While this isn't ideal, I feel this is still acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_train_med, y_pred_class)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice improvement in F-score (51 to 55), while recall oddly increases from 40 to 48 (0 class drops from 95 to 91), and precision improves. Setting the decision threshold at 0.42 clearly improves the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auc\n",
    "print(metrics.roc_auc_score(y_train_med, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "still acceptable AUC, although a drop in performance has occurred..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC for helping with threshold setting\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train_med, y_pred_prob_cut) #y_pred_prob_cut is probability of first group membership (aka 1D array)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.title('ROC curve for 7 day mortality classifier')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to print sensitivity and specificity for different thresholds\n",
    "def evaluate_threshold(threshold):\n",
    "    print('Sensitivity:', tpr[thresholds > threshold][-1])\n",
    "    print('Specificity:', 1 - fpr[thresholds > threshold][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_threshold(0.5) #baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_threshold(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_threshold(0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_threshold(0.42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Training and Test Set results of L1 Logistic Regression\n",
    "<br> \n",
    "I'm feeling like the L1 penalty, with median value imputation (as this yielded the best results in CV testing), top 40 features are the final implementation of this dataset. If more data is found and added, obviously more testing will need to take place, but generally-speaking, the L1 Logistic Regression model is ready to test on the test set, as I'm not considering any tweaks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
