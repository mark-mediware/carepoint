{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#load libraries\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier #baseline model\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "from IPython.display import display #displays full dataframe columns\n",
    "#display all dataframe columns when printed\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271541, 120)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "df = pd.read_csv('C:/Users/Mark.Burghart/Documents/projects/hospice_carepoint/data/transformed/carepoint_transformed_dummied.csv', index_col=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to still create building/holdout sets (80/20), cross validation of building set, impute missing values (set to 0 for now).\n",
    "<br>\n",
    "<br>\n",
    "Pipeline todo: missing data, feature selection, grid search hyperparameter tuning, model training.\n",
    "<br><br>\n",
    "pipe flow: Fill in missing data, feature selection, model training and tuning via grid search. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271541, 119)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#separate variables (X) from outcome of interest (y)\n",
    "df.shape\n",
    "cols = df.columns.get_values() #converts column names to list\n",
    "cols = cols.tolist()\n",
    "feature_cols = [x for x in cols if x != 'death_within_7_days'] #removes outcome of interest from list ('death_within_7_days')\n",
    "\n",
    "#extract rows\n",
    "#print(feature_cols) #debug\n",
    "X = df.loc[:, feature_cols]\n",
    "X.shape #outcome column has been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271541,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save outcome variable as y\n",
    "y = df.death_within_7_days\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate data into training/test (aka holdout) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 23) #random_state for reproducibility (if needed)\n",
    "#X_test, y_test should not be used until NO MORE decisions are being made. This is the final, FINAL validation, and more often just used for model performance and generalizability!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "# Pipepline for imputation, feature selection, and model training/tuning<br><br>\n",
    "#### Logistic Regression with L1 Lasso Penalty, imputation = Median columnar value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First up: Logistic Regression models. Going to try both L1 and L2 Regularization.<br><br>\n",
    "Note: For *MISSING DATA*,  I am **IMPUTING THE MEDIAN COLUMN VALUE**, as opposed to setting to 0, or kNN imputation as of now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbest = SelectKBest(f_classif) #select best 'k' features\n",
    "logreg = LogisticRegression(penalty = 'l1', random_state = 0) #L1 Regularization\n",
    "impute = Imputer(missing_values = 'NaN', strategy = 'median', axis = 0) #impute missing values: replacing NaNs with Median Column value for each column\n",
    "\n",
    "#Pipeline for Logistic Regression with L1 Lasso Regularization\n",
    "pipe_lr_l1 = Pipeline([('imputer', impute),\n",
    "                       ('kbest', kbest),\n",
    "                      ('lr', logreg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for grid search cross validation \n",
    "parameters = {'kbest__k': [40, 60, 80, 100], #building models with 40, 60, 80, and 100 most significant variables\n",
    "                 'lr__C' : [0.01, 0.1, 1, 10, 100]} #tuning C for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search\n",
    "grid = GridSearchCV(pipe_lr_l1, parameters, cv = 5, scoring = 'roc_auc') #run grid on parameters, 5-fold cross validation, AUC is evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:\n",
      "Pipeline(memory=None,\n",
      "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('kbest', SelectKBest(k=100, score_func=<function f_classif at 0x00000238F1E4FAE8>)), ('lr', LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=0, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'logisticregression'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'logisticregression'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#fit models\n",
    "grid.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model interpretations:<br><br>\n",
    "feature [86] is constant, meaning the value is constant for all visits and patients (and should be removed from model).<br><br>\n",
    "true_divide warning is likely due to feature[86], and is trying to divide by 0 variance...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:\n",
      "Pipeline(memory=None,\n",
      "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('kbest', SelectKBest(k=100, score_func=<function f_classif at 0x00000238F1E4FAE8>)), ('lr', LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=0, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "Logistic Regression step:\n",
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=0, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Logistic Regression coefficients:\n",
      "[[-0.14365795  0.02485366 -0.15901603  0.11801389  0.17137557 -0.05839532\n",
      "  -0.00664398  0.08395521  0.05447684  0.09313099 -0.00438136  0.11874033\n",
      "   0.1419672   0.11773742 -0.09116487  0.10354296  0.16950643 -0.02029579\n",
      "  -0.04617929 -0.00820864 -0.01235077  0.04799835  0.03653054 -0.02023392\n",
      "   0.04378721  0.05276292 -0.09788877  0.06554666 -0.13758255  0.\n",
      "  -0.13877318 -0.06101996  0.14396104 -0.08467935 -0.20033684  0.05425719\n",
      "   0.07328142  0.0478336  -0.06813723 -0.03801475 -0.0541456  -0.0953415\n",
      "   0.07162558 -0.10059297 -0.00779695  0.04522752 -0.03854048 -0.04698349\n",
      "   0.02141736 -0.00961538 -0.0087778  -0.02660018  0.00340261 -0.01029848\n",
      "  -0.02985498  0.0118483   0.02807399 -0.01031912  0.00539544  0.04570645\n",
      "   0.07733148 -0.28011341 -1.43886675  0.46005274 -1.50800035 -1.44253629\n",
      "  -1.50549149 -1.69449798 -1.32177609 -1.11721882 -0.89702279  0.09562962\n",
      "  -1.31627486 -1.42100548  0.13979841 -0.20878932 -0.39903513 -0.06632406\n",
      "   0.24539322  0.00730911  0.          0.14348066  0.36385602 -0.7783849\n",
      "   0.33683594  0.21193624  0.63748407  0.61704514  0.80254461  0.46778616\n",
      "  -0.57392948  0.02137192  0.29983013  0.54490418 -0.09800547  0.01101046\n",
      "  -0.70088899 -0.21741753  0.51214364  0.30623327]]\n",
      "Best Parameters: {'kbest__k': 100, 'lr__C': 1}\n",
      "Best cross validation score: 0.82\n"
     ]
    }
   ],
   "source": [
    "print(\"Best estimator:\\n{}\".format(grid.best_estimator_)) #prints best model and pipeline\n",
    "print(\"Logistic Regression step:\\n{}\".format(grid.best_estimator_.named_steps[\"lr\"])) #prints logistic regression step of pipeline\n",
    "print(\"Logistic Regression coefficients:\\n{}\".format(grid.best_estimator_.named_steps[\"lr\"].coef_)) #prints coefficients of best estimator\n",
    "print(\"Best Parameters: {}\".format(grid.best_params_)) #outputs best parameters settings\n",
    "print(\"Best cross validation score: {:.2f}\".format(grid.best_score_)) #best produced cross validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid scores on Development set: \n",
      "0.807 (+/-0.004) for {'kbest__k': 40, 'lr__C': 0.01}\n",
      "0.807 (+/-0.004) for {'kbest__k': 40, 'lr__C': 0.1}\n",
      "0.807 (+/-0.004) for {'kbest__k': 40, 'lr__C': 1}\n",
      "0.807 (+/-0.004) for {'kbest__k': 40, 'lr__C': 10}\n",
      "0.807 (+/-0.004) for {'kbest__k': 40, 'lr__C': 100}\n",
      "0.813 (+/-0.003) for {'kbest__k': 60, 'lr__C': 0.01}\n",
      "0.814 (+/-0.003) for {'kbest__k': 60, 'lr__C': 0.1}\n",
      "0.814 (+/-0.003) for {'kbest__k': 60, 'lr__C': 1}\n",
      "0.814 (+/-0.003) for {'kbest__k': 60, 'lr__C': 10}\n",
      "0.814 (+/-0.003) for {'kbest__k': 60, 'lr__C': 100}\n",
      "0.815 (+/-0.003) for {'kbest__k': 80, 'lr__C': 0.01}\n",
      "0.816 (+/-0.003) for {'kbest__k': 80, 'lr__C': 0.1}\n",
      "0.816 (+/-0.003) for {'kbest__k': 80, 'lr__C': 1}\n",
      "0.816 (+/-0.003) for {'kbest__k': 80, 'lr__C': 10}\n",
      "0.816 (+/-0.003) for {'kbest__k': 80, 'lr__C': 100}\n",
      "0.815 (+/-0.003) for {'kbest__k': 100, 'lr__C': 0.01}\n",
      "0.816 (+/-0.003) for {'kbest__k': 100, 'lr__C': 0.1}\n",
      "0.817 (+/-0.002) for {'kbest__k': 100, 'lr__C': 1}\n",
      "0.817 (+/-0.002) for {'kbest__k': 100, 'lr__C': 10}\n",
      "0.817 (+/-0.002) for {'kbest__k': 100, 'lr__C': 100}\n"
     ]
    }
   ],
   "source": [
    "#AUC scores for each validation set run\n",
    "print(\"Grid scores on Development set: \")\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "\n",
    "for mean, std, params in zip(means, stds, grid.cv_results_['params']): #note imbedded grid_l2 will need to be adjusted to correct object\n",
    "    print(\"%0.3f (+/-%0.3f) for %r\"\n",
    "         % (mean, std *2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression with L1 penalty and median missing data imputation** <br><br>\n",
    "- Best model selected used parameters K = 100 (100 variables included), and C = 1 for LogReg model. \n",
    "- AUC scores ranged from 0.807 (40 variables, C = 0.01-100) to 0.817"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with L2 Ridge Penalty, imputation = Median columnar value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression with L2 Regularization penalty\n",
    "kbest = SelectKBest(f_classif) #select best 'k' features\n",
    "logreg_l2 = LogisticRegression(penalty = 'l2', random_state = 0) #L2 Ridge Regularization\n",
    "impute = Imputer(missing_values = 'NaN', strategy = 'median', axis = 0) #impute missing values: replacing NaNs with Median Column value for each column\n",
    "\n",
    "#Pipeline for Logistic Regression with L2 'Ridge' Regularization\n",
    "pipe_lr_l2 = Pipeline([('imputer', impute),\n",
    "                       ('kbest', kbest),\n",
    "                      ('lr', logreg_l2)])\n",
    "#parameters for grid search cross validation \n",
    "parameters = {'kbest__k': [40, 60, 80, 100], #building models with 40, 60, 80, and 100 most significant variables\n",
    "                 'lr__C' : [0.01, 0.1, 1, 10, 100]} #tuning C for logistic regression\n",
    "\n",
    "#grid search\n",
    "grid_l2 = GridSearchCV(pipe_lr_l2, parameters, cv = 5, scoring = 'roc_auc') #run grid on parameters, 5-fold cross validation, AUC is evaluation metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36min 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('kbest', SelectKBest(k=10, score_func=<function f_classif at 0x00000238F1E4FAE8>)), ('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'kbest__k': [40, 60, 80, 100], 'lr__C': [0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#fit models\n",
    "grid_l2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:\n",
      "Pipeline(memory=None,\n",
      "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('kbest', SelectKBest(k=100, score_func=<function f_classif at 0x00000238F1E4FAE8>)), ('lr', LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "Logistic Regression step:\n",
      "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Logistic Regression coefficients:\n",
      "[[-0.14429463  0.0249696  -0.15859719  0.11811835  0.17114564 -0.05717367\n",
      "  -0.00658466  0.08416823  0.05471053  0.0933325  -0.0043772   0.11885902\n",
      "   0.142829    0.1194861  -0.08918388  0.10400604  0.1702366  -0.01805957\n",
      "  -0.04321872 -0.00806818 -0.01303036  0.04768426  0.03510484 -0.02035464\n",
      "   0.04335001  0.05249293 -0.1002027   0.06249717 -0.13775543 -0.00138401\n",
      "  -0.13985772 -0.06269039  0.14008633 -0.08553619 -0.20204336  0.0511853\n",
      "   0.06926477  0.04930174 -0.06722806 -0.03636132 -0.05322044 -0.09404169\n",
      "   0.07422936 -0.09667218 -0.00800493  0.04520617 -0.03880047 -0.04686499\n",
      "   0.02057873 -0.00983762 -0.00868561 -0.02670657  0.00337031 -0.01056134\n",
      "  -0.03267489  0.01409002  0.0318496  -0.01343944  0.0056167   0.06803618\n",
      "   0.09745828 -0.25607417 -0.38417068  0.09257394 -0.44989692 -0.38763269\n",
      "  -0.45408697 -0.63130784 -0.26014919 -0.06295023 -0.64125908  0.35300692\n",
      "  -1.06718073 -1.16593339  0.14542875 -0.20713336 -0.40186264 -0.0641405\n",
      "   0.2627538   0.00810426 -0.00421043  0.14388285  0.34952487 -0.29459343\n",
      "   0.34139466  0.21992279  0.14755092  0.50951763  0.04766847  0.4784759\n",
      "  -0.40739881  0.03110851  0.30848148  0.55778423 -0.08880626  0.01974804\n",
      "  -0.45537304 -0.20350067  0.5055816   0.31608509]]\n",
      "Best Parameters: {'kbest__k': 100, 'lr__C': 10}\n",
      "Best cross validation score: 0.82\n"
     ]
    }
   ],
   "source": [
    "print(\"Best estimator:\\n{}\".format(grid_l2.best_estimator_)) #prints best model and pipeline\n",
    "print(\"Logistic Regression step:\\n{}\".format(grid_l2.best_estimator_.named_steps[\"lr\"])) #prints logistic regression step of pipeline\n",
    "print(\"Logistic Regression coefficients:\\n{}\".format(grid_l2.best_estimator_.named_steps[\"lr\"].coef_)) #prints coefficients of best estimator\n",
    "print(\"Best Parameters: {}\".format(grid_l2.best_params_)) #outputs best parameters settings\n",
    "print(\"Best cross validation score: {:.2f}\".format(grid_l2.best_score_)) #best produced cross validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid scores on Development set: \n",
      "0.807 (+/-0.004) for {'kbest__k': 40, 'lr__C': 0.01}\n",
      "0.807 (+/-0.004) for {'kbest__k': 40, 'lr__C': 0.1}\n",
      "0.807 (+/-0.004) for {'kbest__k': 40, 'lr__C': 1}\n",
      "0.807 (+/-0.004) for {'kbest__k': 40, 'lr__C': 10}\n",
      "0.807 (+/-0.004) for {'kbest__k': 40, 'lr__C': 100}\n",
      "0.813 (+/-0.003) for {'kbest__k': 60, 'lr__C': 0.01}\n",
      "0.814 (+/-0.003) for {'kbest__k': 60, 'lr__C': 0.1}\n",
      "0.814 (+/-0.003) for {'kbest__k': 60, 'lr__C': 1}\n",
      "0.814 (+/-0.003) for {'kbest__k': 60, 'lr__C': 10}\n",
      "0.814 (+/-0.003) for {'kbest__k': 60, 'lr__C': 100}\n",
      "0.815 (+/-0.003) for {'kbest__k': 80, 'lr__C': 0.01}\n",
      "0.816 (+/-0.003) for {'kbest__k': 80, 'lr__C': 0.1}\n",
      "0.816 (+/-0.003) for {'kbest__k': 80, 'lr__C': 1}\n",
      "0.816 (+/-0.003) for {'kbest__k': 80, 'lr__C': 10}\n",
      "0.816 (+/-0.003) for {'kbest__k': 80, 'lr__C': 100}\n",
      "0.816 (+/-0.003) for {'kbest__k': 100, 'lr__C': 0.01}\n",
      "0.817 (+/-0.003) for {'kbest__k': 100, 'lr__C': 0.1}\n",
      "0.817 (+/-0.002) for {'kbest__k': 100, 'lr__C': 1}\n",
      "0.817 (+/-0.002) for {'kbest__k': 100, 'lr__C': 10}\n",
      "0.817 (+/-0.002) for {'kbest__k': 100, 'lr__C': 100}\n"
     ]
    }
   ],
   "source": [
    "#AUC scores for each validation set run\n",
    "\n",
    "print(\"Grid scores on Development set: \")\n",
    "means = grid_l2.cv_results_['mean_test_score']\n",
    "stds = grid_l2.cv_results_['std_test_score']\n",
    "\n",
    "for mean, std, params in zip(means, stds, grid_l2.cv_results_['params']): #note imbedded grid_l2 will need to be adjusted to correct object\n",
    "    print(\"%0.3f (+/-%0.3f) for %r\"\n",
    "         % (mean, std *2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Br>**Logistic Regression with L2 penalty and median missing data imputation**<br><br>\n",
    "- Best model selected used parameters K = 100 (100 variables included), and C = 10 for LogReg model. <br>\n",
    "- AUC scores ranged from 0.807 (40 variables, C = 0.01-100) to 0.817 \n",
    "- Interestingly, these outputs produced the same AUC across all cross validation trials as L1 model.\n",
    "    -Only difference was C = 1 for L1. \n",
    "    - Could be due to same random_state parameter?<br>\n",
    "    <br>\n",
    "    \n",
    "    Also interesting to note that adding 60 additional variables only improves AUC by ~1%. 100 variables is likely overfitting...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
