{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier #baseline model\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "from IPython.display import display #displays full dataframe columns\n",
    "#display all dataframe columns when printed\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271541, 120)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "df = pd.read_csv('C:/Users/Mark.Burghart/Documents/projects/hospice_carepoint/data/transformed/carepoint_transformed_dummied.csv', index_col=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to still create building/holdout sets (80/20), cross validation of building set, impute missing values (set to 0 for now).\n",
    "<br>\n",
    "<br>\n",
    "Pipeline todo: missing data, feature selection, grid search hyperparameter tuning, model training.\n",
    "<br><br>\n",
    "pipe flow: Fill in missing data, feature selection, model training and tuning via grid search. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271541, 119)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#separate variables (X) from outcome of interest (y)\n",
    "df.shape\n",
    "cols = df.columns.get_values() #converts column names to list\n",
    "cols = cols.tolist()\n",
    "feature_cols = [x for x in cols if x != 'death_within_7_days'] #removes outcome of interest from list ('death_within_7_days')\n",
    "\n",
    "#extract rows\n",
    "#print(feature_cols) #debug\n",
    "X = df.loc[:, feature_cols]\n",
    "X.shape #outcome column has been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271541,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save outcome variable as y\n",
    "y = df.death_within_7_days\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate data into training/test (aka holdout) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 23) #random_state for reproducibility (if needed)\n",
    "#X_test, y_test should not be used until NO MORE decisions are being made. This is the final, FINAL validation, and more often just used for model performance and generalizability!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "# Pipepline for imputation, feature selection, and model training/tuning<br><br>\n",
    "#### Logistic Regression with L1 Lasso Penalty, imputation = Median columnar value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First up: Logistic Regression models. Going to try both L1 and L2 Regularization.<br><br>\n",
    "Note: For *MISSING DATA*,  I am **IMPUTING THE MEDIAN COLUMN VALUE**, as opposed to setting to 0, or kNN imputation as of now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbest = SelectKBest(f_classif) #select best 'k' features\n",
    "logreg = LogisticRegression(penalty = 'l1', random_state = 0) #L1 Regularization\n",
    "impute = Imputer(missing_values = 'NaN', strategy = 'median', axis = 0) #impute missing values: replacing NaNs with Median Column value for each column\n",
    "\n",
    "#Pipeline for Logistic Regression with L1 Lasso Regularization\n",
    "pipe_lr_l1 = Pipeline([('imputer', impute),\n",
    "                       ('kbest', kbest),\n",
    "                      ('lr', logreg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for grid search cross validation \n",
    "parameters = {'kbest__k': [5, 10, 20,40, 60], #building models with 40, 60, 80, and 100 most significant variables\n",
    "                 'lr__C' : [0.01, 0.1, 1, 10, 100]} #tuning C for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search\n",
    "grid = GridSearchCV(pipe_lr_l1, parameters, cv = 5, scoring = 'roc_auc') #run grid on parameters, 5-fold cross validation, AUC is evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:\n",
      "Pipeline(memory=None,\n",
      "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('kbest', SelectKBest(k=100, score_func=<function f_classif at 0x00000238F1E4FAE8>)), ('lr', LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=0, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'logisticregression'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'logisticregression'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#fit models\n",
    "grid.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model interpretations:<br><br>\n",
    "feature [86] is constant, meaning the value is constant for all visits and patients (and should be removed from model).<br><br>\n",
    "true_divide warning is likely due to feature[86], and is trying to divide by 0 variance...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:\n",
      "Pipeline(memory=None,\n",
      "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('kbest', SelectKBest(k=100, score_func=<function f_classif at 0x00000238F1E4FAE8>)), ('lr', LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=0, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "Logistic Regression step:\n",
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=0, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Logistic Regression coefficients:\n",
      "[[-0.14365795  0.02485366 -0.15901603  0.11801389  0.17137557 -0.05839532\n",
      "  -0.00664398  0.08395521  0.05447684  0.09313099 -0.00438136  0.11874033\n",
      "   0.1419672   0.11773742 -0.09116487  0.10354296  0.16950643 -0.02029579\n",
      "  -0.04617929 -0.00820864 -0.01235077  0.04799835  0.03653054 -0.02023392\n",
      "   0.04378721  0.05276292 -0.09788877  0.06554666 -0.13758255  0.\n",
      "  -0.13877318 -0.06101996  0.14396104 -0.08467935 -0.20033684  0.05425719\n",
      "   0.07328142  0.0478336  -0.06813723 -0.03801475 -0.0541456  -0.0953415\n",
      "   0.07162558 -0.10059297 -0.00779695  0.04522752 -0.03854048 -0.04698349\n",
      "   0.02141736 -0.00961538 -0.0087778  -0.02660018  0.00340261 -0.01029848\n",
      "  -0.02985498  0.0118483   0.02807399 -0.01031912  0.00539544  0.04570645\n",
      "   0.07733148 -0.28011341 -1.43886675  0.46005274 -1.50800035 -1.44253629\n",
      "  -1.50549149 -1.69449798 -1.32177609 -1.11721882 -0.89702279  0.09562962\n",
      "  -1.31627486 -1.42100548  0.13979841 -0.20878932 -0.39903513 -0.06632406\n",
      "   0.24539322  0.00730911  0.          0.14348066  0.36385602 -0.7783849\n",
      "   0.33683594  0.21193624  0.63748407  0.61704514  0.80254461  0.46778616\n",
      "  -0.57392948  0.02137192  0.29983013  0.54490418 -0.09800547  0.01101046\n",
      "  -0.70088899 -0.21741753  0.51214364  0.30623327]]\n",
      "Best Parameters: {'kbest__k': 100, 'lr__C': 1}\n",
      "Best cross validation score: 0.82\n"
     ]
    }
   ],
   "source": [
    "print(\"Best estimator:\\n{}\".format(grid.best_estimator_)) #prints best model and pipeline\n",
    "print(\"Logistic Regression step:\\n{}\".format(grid.best_estimator_.named_steps[\"lr\"])) #prints logistic regression step of pipeline\n",
    "print(\"Logistic Regression coefficients:\\n{}\".format(grid.best_estimator_.named_steps[\"lr\"].coef_)) #prints coefficients of best estimator\n",
    "print(\"Best Parameters: {}\".format(grid.best_params_)) #outputs best parameters settings\n",
    "print(\"Best cross validation score: {:.2f}\".format(grid.best_score_)) #best produced cross validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid scores on training set: \n",
      "0.807 (+/-0.001) for {'kbest__k': 40, 'lr__C': 0.01}\n",
      "0.807 (+/-0.001) for {'kbest__k': 40, 'lr__C': 0.1}\n",
      "0.807 (+/-0.001) for {'kbest__k': 40, 'lr__C': 1}\n",
      "0.807 (+/-0.001) for {'kbest__k': 40, 'lr__C': 10}\n",
      "0.807 (+/-0.001) for {'kbest__k': 40, 'lr__C': 100}\n",
      "0.813 (+/-0.001) for {'kbest__k': 60, 'lr__C': 0.01}\n",
      "0.814 (+/-0.001) for {'kbest__k': 60, 'lr__C': 0.1}\n",
      "0.814 (+/-0.001) for {'kbest__k': 60, 'lr__C': 1}\n",
      "0.814 (+/-0.001) for {'kbest__k': 60, 'lr__C': 10}\n",
      "0.814 (+/-0.001) for {'kbest__k': 60, 'lr__C': 100}\n",
      "0.815 (+/-0.001) for {'kbest__k': 80, 'lr__C': 0.01}\n",
      "0.816 (+/-0.001) for {'kbest__k': 80, 'lr__C': 0.1}\n",
      "0.816 (+/-0.001) for {'kbest__k': 80, 'lr__C': 1}\n",
      "0.816 (+/-0.001) for {'kbest__k': 80, 'lr__C': 10}\n",
      "0.816 (+/-0.001) for {'kbest__k': 80, 'lr__C': 100}\n",
      "0.816 (+/-0.001) for {'kbest__k': 100, 'lr__C': 0.01}\n",
      "0.817 (+/-0.001) for {'kbest__k': 100, 'lr__C': 0.1}\n",
      "0.817 (+/-0.001) for {'kbest__k': 100, 'lr__C': 1}\n",
      "0.817 (+/-0.001) for {'kbest__k': 100, 'lr__C': 10}\n",
      "0.817 (+/-0.001) for {'kbest__k': 100, 'lr__C': 100}\n",
      "Grid scores on Development set: \n",
      "0.807 (+/-0.004) for {'kbest__k': 40, 'lr__C': 0.01}\n",
      "0.807 (+/-0.004) for {'kbest__k': 40, 'lr__C': 0.1}\n",
      "0.807 (+/-0.004) for {'kbest__k': 40, 'lr__C': 1}\n",
      "0.807 (+/-0.004) for {'kbest__k': 40, 'lr__C': 10}\n",
      "0.807 (+/-0.004) for {'kbest__k': 40, 'lr__C': 100}\n",
      "0.813 (+/-0.003) for {'kbest__k': 60, 'lr__C': 0.01}\n",
      "0.814 (+/-0.003) for {'kbest__k': 60, 'lr__C': 0.1}\n",
      "0.814 (+/-0.003) for {'kbest__k': 60, 'lr__C': 1}\n",
      "0.814 (+/-0.003) for {'kbest__k': 60, 'lr__C': 10}\n",
      "0.814 (+/-0.003) for {'kbest__k': 60, 'lr__C': 100}\n",
      "0.815 (+/-0.003) for {'kbest__k': 80, 'lr__C': 0.01}\n",
      "0.816 (+/-0.003) for {'kbest__k': 80, 'lr__C': 0.1}\n",
      "0.816 (+/-0.003) for {'kbest__k': 80, 'lr__C': 1}\n",
      "0.816 (+/-0.003) for {'kbest__k': 80, 'lr__C': 10}\n",
      "0.816 (+/-0.003) for {'kbest__k': 80, 'lr__C': 100}\n",
      "0.815 (+/-0.003) for {'kbest__k': 100, 'lr__C': 0.01}\n",
      "0.816 (+/-0.003) for {'kbest__k': 100, 'lr__C': 0.1}\n",
      "0.817 (+/-0.002) for {'kbest__k': 100, 'lr__C': 1}\n",
      "0.817 (+/-0.002) for {'kbest__k': 100, 'lr__C': 10}\n",
      "0.817 (+/-0.002) for {'kbest__k': 100, 'lr__C': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "#AUC scores for each training set run\n",
    "print(\"Grid scores on training set: \")\n",
    "means = grid.cv_results_['mean_train_score']\n",
    "stds = grid.cv_results_['std_train_score']\n",
    "\n",
    "for mean, std, params in zip(means, stds, grid.cv_results_['params']): #note imbedded grid_l2 will need to be adjusted to correct object\n",
    "    print(\"%0.3f (+/-%0.3f) for %r\"\n",
    "         % (mean, std *2, params))\n",
    "    \n",
    "#AUC scores for each validation set run\n",
    "print(\"Grid scores on Development set: \")\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "\n",
    "for mean, std, params in zip(means, stds, grid.cv_results_['params']): #note imbedded grid_l2 will need to be adjusted to correct object\n",
    "    print(\"%0.3f (+/-%0.3f) for %r\"\n",
    "         % (mean, std *2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-dc98888b9fea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mselector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_classif\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mX_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimputed_X_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_new\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mcols_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "#return variables of significance\n",
    "#start with 40\n",
    "fill_NaN = Imputer(missing_values=np.nan, strategy='median', axis=0)\n",
    "imputed_X_train = pd.DataFrame(fill_NaN.fit_transform(X_train))\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_train.index = X_train.index\n",
    "\n",
    "#display columns..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['gender', 'Anxiety', 'Depression', 'Drowsiness', 'Lack_of_Appetite',\n",
      "       'Nausea', 'Pain', 'Shortness_of_Breath', 'Tiredness', 'Wellbeing',\n",
      "       'LengthOfCare_days', '3_visit_max_anxiety', '3_visit_max_drowsiness',\n",
      "       '3_visit_max_lackofappetite', '3_visit_max_nausea', '3_visit_max_pain',\n",
      "       '3_visit_max_shortnessofbreath', '3_visit_max_tiredness',\n",
      "       '3_visit_max_wellbeing', '5_visit_max_anxiety',\n",
      "       '5_visit_max_depression', '5_visit_max_drowsiness',\n",
      "       '5_visit_max_lackofappetite', '5_visit_max_nausea', '5_visit_max_pain',\n",
      "       '5_visit_max_shortnessofbreath', '5_visit_max_tiredness',\n",
      "       '5_visit_max_wellbeing', '3_visit_mean_anxiety',\n",
      "       '3_visit_mean_depression', '3_visit_mean_drowsiness',\n",
      "       '3_visit_mean_lackofappetite', '3_visit_mean_nausea',\n",
      "       '3_visit_mean_pain', '3_visit_mean_shortnessofbreath',\n",
      "       '3_visit_mean_tiredness', '3_visit_mean_wellbeing',\n",
      "       '5_visit_mean_depression', '5_visit_mean_drowsiness',\n",
      "       '5_visit_mean_lackofappetite', '5_visit_mean_pain',\n",
      "       '5_visit_mean_shortnessofbreath', '5_visit_mean_tiredness',\n",
      "       '5_visit_mean_wellbeing', 'Anxiety_change', 'Depression_change',\n",
      "       'Drowsiness_change', 'LackofAppetite_change', 'Nausea_change',\n",
      "       'ShortnessofBreath_change', 'Tiredness_change', 'Wellbeing_change',\n",
      "       'Age', 'ESAS_visit_total', '3_visit_max_esas', '5_visit_max_esas',\n",
      "       '3_visit_mean_esas', '5_visit_mean_esas', 'ESAS_change',\n",
      "       'AdvanceDirective_ No',\n",
      "       'AdvanceDirective_Yes - Do Not Resuscitate (DNR)',\n",
      "       'AdvanceDirective_Yes - Full Code / Advanced Cardiac Life Support (ACLS)',\n",
      "       'ReferralType_Clinic or physician's office',\n",
      "       'ReferralType_Court/Law Enforcement',\n",
      "       'ReferralType_Information not available',\n",
      "       'ReferralType_Non-health care facility',\n",
      "       'ReferralType_Transfer from Home Health Agency',\n",
      "       'ReferralType_Transfer from Hospice',\n",
      "       'ReferralType_Transfer from SNF or ICF',\n",
      "       'ReferralType_Transfer from hospital', 'LevelofCare_Continuous (CHC)',\n",
      "       'LevelofCare_Inpatient (GIP)', 'LevelofCare_Respite',\n",
      "       'LevelofCare_Routine', 'Race_American Indian or Alaskan Native',\n",
      "       'Race_Asian', 'Race_Black or African American',\n",
      "       'Race_Hispanic or Latino', 'Race_Native Hawaiian or Pacific Islander',\n",
      "       'Race_White', 'InsuranceType_Medicaid (HMO/Managed Care)',\n",
      "       'InsuranceType_Medicare Traditional', 'InsuranceType_Other',\n",
      "       'InsuranceType_Other Government',\n",
      "       'InsuranceType_Private HMO/Managed Care',\n",
      "       'InsuranceType_Private Insurance', 'InsuranceType_Private Pay',\n",
      "       'InsuranceType_Self-Pay',\n",
      "       'icd10_cluster_Certain conditions originating in the perinatal period',\n",
      "       'icd10_cluster_Certain infectious and parasitic diseases',\n",
      "       'icd10_cluster_Congenital malformations, deformations and chromosomal abnormalities',\n",
      "       'icd10_cluster_Diseases of the circulatory system',\n",
      "       'icd10_cluster_Diseases of the digestive system',\n",
      "       'icd10_cluster_Diseases of the genitourinary system',\n",
      "       'icd10_cluster_Diseases of the nervous system',\n",
      "       'icd10_cluster_Diseases of the respiratory system',\n",
      "       'icd10_cluster_Diseases of the skin and subcutaneous tissue',\n",
      "       'icd10_cluster_Endocrine, nutritional, and metabolic diseases',\n",
      "       'icd10_cluster_Injury, poisonining, and certain other consequences of external causes',\n",
      "       'icd10_cluster_Neoplasms'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#return variables of significance\n",
    "#Return 100 features used in best model\n",
    "selector = SelectKBest(f_classif, k = 100)\n",
    "selector.fit(imputed_X_train, y_train)\n",
    "features = imputed_X_train.columns[selector.get_support()]\n",
    "print(features)\n",
    "scores = selector.scores_ #saves scores of features\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression with L1 penalty and median missing data imputation** <br><br>\n",
    "- Best model selected used parameters K = 100 (100 variables included), and C = 1 for LogReg model. \n",
    "- AUC scores ranged from 0.807 (40 variables, C = 0.01-100) to 0.817"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with L2 Ridge Penalty, imputation = Median columnar value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression with L2 Regularization penalty\n",
    "kbest = SelectKBest(f_classif) #select best 'k' features\n",
    "logreg_l2 = LogisticRegression(penalty = 'l2', random_state = 0) #L2 Ridge Regularization\n",
    "impute = Imputer(missing_values = 'NaN', strategy = 'median', axis = 0) #impute missing values: replacing NaNs with Median Column value for each column\n",
    "\n",
    "#Pipeline for Logistic Regression with L2 'Ridge' Regularization\n",
    "pipe_lr_l2 = Pipeline([('imputer', impute),\n",
    "                       ('kbest', kbest),\n",
    "                      ('lr', logreg_l2)])\n",
    "#parameters for grid search cross validation \n",
    "parameters = {'kbest__k': [5, 10, 20, 40, 60], #building models with 40, 60, 80, and 100 most significant variables\n",
    "                 'lr__C' : [0.01, 0.1, 1, 10, 100]} #tuning C for logistic regression\n",
    "\n",
    "#grid search\n",
    "grid_l2 = GridSearchCV(pipe_lr_l2, parameters, cv = 5, scoring = 'roc_auc') #run grid on parameters, 5-fold cross validation, AUC is evaluation metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 36min 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('kbest', SelectKBest(k=10, score_func=<function f_classif at 0x00000238F1E4FAE8>)), ('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'kbest__k': [40, 60, 80, 100], 'lr__C': [0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#fit models\n",
    "grid_l2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:\n",
      "Pipeline(memory=None,\n",
      "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('kbest', SelectKBest(k=100, score_func=<function f_classif at 0x00000238F1E4FAE8>)), ('lr', LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "Logistic Regression step:\n",
      "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Logistic Regression coefficients:\n",
      "[[-0.14429463  0.0249696  -0.15859719  0.11811835  0.17114564 -0.05717367\n",
      "  -0.00658466  0.08416823  0.05471053  0.0933325  -0.0043772   0.11885902\n",
      "   0.142829    0.1194861  -0.08918388  0.10400604  0.1702366  -0.01805957\n",
      "  -0.04321872 -0.00806818 -0.01303036  0.04768426  0.03510484 -0.02035464\n",
      "   0.04335001  0.05249293 -0.1002027   0.06249717 -0.13775543 -0.00138401\n",
      "  -0.13985772 -0.06269039  0.14008633 -0.08553619 -0.20204336  0.0511853\n",
      "   0.06926477  0.04930174 -0.06722806 -0.03636132 -0.05322044 -0.09404169\n",
      "   0.07422936 -0.09667218 -0.00800493  0.04520617 -0.03880047 -0.04686499\n",
      "   0.02057873 -0.00983762 -0.00868561 -0.02670657  0.00337031 -0.01056134\n",
      "  -0.03267489  0.01409002  0.0318496  -0.01343944  0.0056167   0.06803618\n",
      "   0.09745828 -0.25607417 -0.38417068  0.09257394 -0.44989692 -0.38763269\n",
      "  -0.45408697 -0.63130784 -0.26014919 -0.06295023 -0.64125908  0.35300692\n",
      "  -1.06718073 -1.16593339  0.14542875 -0.20713336 -0.40186264 -0.0641405\n",
      "   0.2627538   0.00810426 -0.00421043  0.14388285  0.34952487 -0.29459343\n",
      "   0.34139466  0.21992279  0.14755092  0.50951763  0.04766847  0.4784759\n",
      "  -0.40739881  0.03110851  0.30848148  0.55778423 -0.08880626  0.01974804\n",
      "  -0.45537304 -0.20350067  0.5055816   0.31608509]]\n",
      "Best Parameters: {'kbest__k': 100, 'lr__C': 10}\n",
      "Best cross validation score: 0.82\n"
     ]
    }
   ],
   "source": [
    "print(\"Best estimator:\\n{}\".format(grid_l2.best_estimator_)) #prints best model and pipeline\n",
    "print(\"Logistic Regression step:\\n{}\".format(grid_l2.best_estimator_.named_steps[\"lr\"])) #prints logistic regression step of pipeline\n",
    "print(\"Logistic Regression coefficients:\\n{}\".format(grid_l2.best_estimator_.named_steps[\"lr\"].coef_)) #prints coefficients of best estimator\n",
    "print(\"Best Parameters: {}\".format(grid_l2.best_params_)) #outputs best parameters settings\n",
    "print(\"Best cross validation score: {:.2f}\".format(grid_l2.best_score_)) #best produced cross validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid scores on Development set: \n",
      "0.807 (+/-0.001) for {'kbest__k': 40, 'lr__C': 0.01}\n",
      "0.807 (+/-0.001) for {'kbest__k': 40, 'lr__C': 0.1}\n",
      "0.807 (+/-0.001) for {'kbest__k': 40, 'lr__C': 1}\n",
      "0.807 (+/-0.001) for {'kbest__k': 40, 'lr__C': 10}\n",
      "0.807 (+/-0.001) for {'kbest__k': 40, 'lr__C': 100}\n",
      "0.813 (+/-0.001) for {'kbest__k': 60, 'lr__C': 0.01}\n",
      "0.814 (+/-0.001) for {'kbest__k': 60, 'lr__C': 0.1}\n",
      "0.814 (+/-0.001) for {'kbest__k': 60, 'lr__C': 1}\n",
      "0.814 (+/-0.001) for {'kbest__k': 60, 'lr__C': 10}\n",
      "0.814 (+/-0.001) for {'kbest__k': 60, 'lr__C': 100}\n",
      "0.815 (+/-0.001) for {'kbest__k': 80, 'lr__C': 0.01}\n",
      "0.816 (+/-0.001) for {'kbest__k': 80, 'lr__C': 0.1}\n",
      "0.816 (+/-0.001) for {'kbest__k': 80, 'lr__C': 1}\n",
      "0.816 (+/-0.001) for {'kbest__k': 80, 'lr__C': 10}\n",
      "0.816 (+/-0.001) for {'kbest__k': 80, 'lr__C': 100}\n",
      "0.816 (+/-0.001) for {'kbest__k': 100, 'lr__C': 0.01}\n",
      "0.817 (+/-0.001) for {'kbest__k': 100, 'lr__C': 0.1}\n",
      "0.817 (+/-0.001) for {'kbest__k': 100, 'lr__C': 1}\n",
      "0.817 (+/-0.001) for {'kbest__k': 100, 'lr__C': 10}\n",
      "0.817 (+/-0.001) for {'kbest__k': 100, 'lr__C': 100}\n",
      "Grid scores on Development set: \n",
      "0.807 (+/-0.004) for {'kbest__k': 40, 'lr__C': 0.01}\n",
      "0.807 (+/-0.004) for {'kbest__k': 40, 'lr__C': 0.1}\n",
      "0.807 (+/-0.004) for {'kbest__k': 40, 'lr__C': 1}\n",
      "0.807 (+/-0.004) for {'kbest__k': 40, 'lr__C': 10}\n",
      "0.807 (+/-0.004) for {'kbest__k': 40, 'lr__C': 100}\n",
      "0.813 (+/-0.003) for {'kbest__k': 60, 'lr__C': 0.01}\n",
      "0.814 (+/-0.003) for {'kbest__k': 60, 'lr__C': 0.1}\n",
      "0.814 (+/-0.003) for {'kbest__k': 60, 'lr__C': 1}\n",
      "0.814 (+/-0.003) for {'kbest__k': 60, 'lr__C': 10}\n",
      "0.814 (+/-0.003) for {'kbest__k': 60, 'lr__C': 100}\n",
      "0.815 (+/-0.003) for {'kbest__k': 80, 'lr__C': 0.01}\n",
      "0.816 (+/-0.003) for {'kbest__k': 80, 'lr__C': 0.1}\n",
      "0.816 (+/-0.003) for {'kbest__k': 80, 'lr__C': 1}\n",
      "0.816 (+/-0.003) for {'kbest__k': 80, 'lr__C': 10}\n",
      "0.816 (+/-0.003) for {'kbest__k': 80, 'lr__C': 100}\n",
      "0.816 (+/-0.003) for {'kbest__k': 100, 'lr__C': 0.01}\n",
      "0.817 (+/-0.003) for {'kbest__k': 100, 'lr__C': 0.1}\n",
      "0.817 (+/-0.002) for {'kbest__k': 100, 'lr__C': 1}\n",
      "0.817 (+/-0.002) for {'kbest__k': 100, 'lr__C': 10}\n",
      "0.817 (+/-0.002) for {'kbest__k': 100, 'lr__C': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "#AUC scores for each training set run\n",
    "print(\"Grid scores on Development set: \")\n",
    "means = grid.cv_results_['mean_train_score']\n",
    "stds = grid.cv_results_['std_train_score']\n",
    "\n",
    "for mean, std, params in zip(means, stds, grid.cv_results_['params']): #note imbedded grid_l2 will need to be adjusted to correct object\n",
    "    print(\"%0.3f (+/-%0.3f) for %r\"\n",
    "         % (mean, std *2, params))\n",
    "\n",
    "#AUC scores for each validation set run\n",
    "print(\"Grid scores on Development set: \")\n",
    "means = grid_l2.cv_results_['mean_test_score']\n",
    "stds = grid_l2.cv_results_['std_test_score']\n",
    "\n",
    "for mean, std, params in zip(means, stds, grid_l2.cv_results_['params']): #note imbedded grid_l2 will need to be adjusted to correct object\n",
    "    print(\"%0.3f (+/-%0.3f) for %r\"\n",
    "         % (mean, std *2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Br>**Logistic Regression with L2 penalty and median missing data imputation**<br><br>\n",
    "- Best model selected used parameters K = 100 (100 variables included), and C = 10 for LogReg model. <br>\n",
    "- AUC scores ranged from 0.807 (40 variables, C = 0.01-100) to 0.817 \n",
    "- Interestingly, these outputs produced the same AUC across all cross validation trials as L1 model.\n",
    "    -Only difference was C = 1 for L1. \n",
    "    - Could be due to same random_state parameter?<br>\n",
    "    <br>\n",
    "    \n",
    "    Also interesting to note that adding 60 additional variables only improves AUC by ~1%. 100 variables is likely overfitting...\n",
    "    <br><Br>\n",
    "    Not sure why the training and test validation scores are exactly the same...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kNearest Neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, clf, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m                 \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KNeighborsClassifier' object has no attribute 'decision_function'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[0;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[0;32m    343\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             return_times=True)\n\u001b[1;32m--> 206\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    486\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[1;31m# _score will return dict if is_multimetric is True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[0mtest_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_score\u001b[1;34m(estimator, X_test, y_test, scorer, is_multimetric)\u001b[0m\n\u001b[0;32m    521\u001b[0m     \"\"\"\n\u001b[0;32m    522\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_multimetric_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_multimetric_score\u001b[1;34m(estimator, X_test, y_test, scorers)\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'item'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, clf, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mNotImplementedError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m                 \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\classification.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m         \u001b[0mneigh_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    383\u001b[0m                 delayed(self._tree.query, check_pickle=False)(\n\u001b[0;32m    384\u001b[0m                     X[s], n_neighbors, return_distance)\n\u001b[1;32m--> 385\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m             )\n\u001b[0;32m    387\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# creating odd list of K for KNN\n",
    "neighbors = [1,3,5,7]\n",
    "\n",
    "# empty list that will hold cv scores\n",
    "cv_scores = []\n",
    "\n",
    "# perform 5-fold cross validation\n",
    "for k in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, imputed_X_train, y_train, cv=5, scoring='roc_auc') #using median imputed dataset for training\n",
    "    cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(\"The optimal number of neighbors is %d\" % optimal_k)? (<ipython-input-131-ddd77f5c6104>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-131-ddd77f5c6104>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    print \"The optimal number of neighbors is %d\" % optimal_k\u001b[0m\n\u001b[1;37m                                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(\"The optimal number of neighbors is %d\" % optimal_k)?\n"
     ]
    }
   ],
   "source": [
    "# changing to misclassification error\n",
    "MSE = [1 - x for x in cv_scores]\n",
    "\n",
    "# determining best k\n",
    "optimal_k = neighbors[MSE.index(min(MSE))]\n",
    "print(\"The optimal number of neighbors is %d\" % optimal_k)\n",
    "\n",
    "# plot misclassification error vs k\n",
    "plt.plot(neighbors, MSE)\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('Misclassification Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision tree model\n",
    "tree = DecisionTreeClassifier(random_state = 0) #maximum depth of 5 leaves\n",
    "kbest = SelectKBest(f_classif) #select best 'k' features\n",
    "impute = Imputer(missing_values = 'NaN', strategy = 'median', axis = 0) #impute missing values: replacing NaNs with Median Column value for each column\n",
    "\n",
    "#Pipeline for Logistic Regression with L2 'Ridge' Regularization\n",
    "pipe_tree = Pipeline([('imputer', impute),\n",
    "                       ('kbest', kbest),\n",
    "                      ('dec_tree', tree)])\n",
    "#parameters for grid search cross validation \n",
    "parameters = {'kbest__k': [5, 10, 20, 40, 60], #building models with 40, 60, 80, and 100 most significant variables\n",
    "                 'dec_tree__max_depth' : [2,3,4,5]} #tuning tree depth for performance vs understanding\n",
    "\n",
    "#grid search\n",
    "grid_tree = GridSearchCV(pipe_tree, parameters, cv = 5, scoring = 'roc_auc') #run grid on parameters, 5-fold cross validation, AUC is evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#fit models\n",
    "grid_tree.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best estimator:\\n{}\".format(grid_tree.best_estimator_)) #prints best model and pipeline\n",
    "print(\"Logistic Regression step:\\n{}\".format(grid_tree.best_estimator_.named_steps[\"dec_tree\"])) #prints logistic regression step of pipeline\n",
    "print(\"Logistic Regression coefficients:\\n{}\".format(grid_tree.best_estimator_.named_steps[\"dec_tree\"].coef_)) #prints coefficients of best estimator\n",
    "print(\"Best Parameters: {}\".format(grid_tree.best_params_)) #outputs best parameters settings\n",
    "print(\"Best cross validation score: {:.2f}\".format(grid_tree.best_score_)) #best produced cross validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
