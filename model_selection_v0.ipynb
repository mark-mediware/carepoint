{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection v0 work<br>\n",
    "This notebook details the model selection process for the 7 day mortality risk prediction model to be used with Carepoint Hospice Tool. The models evaluated in below include Logistic Regression with L1 Lasso regularization, Logistic Regression with L2 Ridge regularization, k Nearest Neighbors classifiers, and a Decision Tree classifier. Feature selection was automated in a sklearn pipeline using GridSearchCV(), with the cross validation set at 5 folds. Additionally, 2 methods for missing data were evaluated for each potential model and hyperparameter setting: Imputation of the column medians for each missing value, and all missing values set at 0.<BR><BR>\n",
    "    **Objectives:**<br>\n",
    "1. Identify optimal hyperparameter structure for models.<br>\n",
    "2. Determine most appropriate missing data imputation method. <br>\n",
    "3. Explore the impact the number of features has on model performance.<br>\n",
    "4. Identify AUC target for first iteration of Risk Model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier #baseline model\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "from IPython.display import display #displays full dataframe columns\n",
    "#display all dataframe columns when printed\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271541, 120)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "df = pd.read_csv('C:/Users/Mark.Burghart/Documents/projects/hospice_carepoint/data/transformed/carepoint_transformed_dummied.csv', index_col=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split<br><br>\n",
    "Splitting main dataframe by 70/30 split, leaving ~190,000 visits for training/cross validation. Remaining 30% of data will be held out from all model building, hyperparamenter tuning, and feature selection and will be solely used to determine the final model's performance. After model is tested on holdout set, no changes to model will be made.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271541, 119)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#separate variables (X) from outcome of interest (y)\n",
    "df.shape\n",
    "cols = df.columns.get_values() #converts column names to list\n",
    "cols = cols.tolist()\n",
    "feature_cols = [x for x in cols if x != 'death_within_7_days'] #removes outcome of interest from list ('death_within_7_days')\n",
    "\n",
    "#extract rows\n",
    "#print(feature_cols) #debug\n",
    "X = df.loc[:, feature_cols]\n",
    "X.shape #outcome column has been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271541,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save outcome variable as y\n",
    "y = df.death_within_7_days\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate data into training/test (aka holdout) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 23) #random_state for reproducibility (if needed)\n",
    "#X_test, y_test should not be used until NO MORE decisions are being made. This is the final, FINAL validation, and more often just used for model performance and generalizability!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "# Pipepline for imputation, feature selection, and model training/tuning<br><br>\n",
    "### Missing Data: Median imputation<br><br>\n",
    "\n",
    "#### Logistic Regression with L1 Lasso Penalty, imputation = Median columnar value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First up: Logistic Regression models. Going to try both L1 and L2 Regularization.<br><br>\n",
    "Note: For *MISSING DATA*,  I am **IMPUTING THE MEDIAN COLUMN VALUE**, as opposed to setting to 0, or kNN imputation as of now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbest = SelectKBest(f_classif) #select best 'k' features\n",
    "logreg = LogisticRegression(penalty = 'l1', random_state = 0) #L1 Regularization\n",
    "impute = Imputer(missing_values = 'NaN', strategy = 'median', axis = 0) #impute missing values: replacing NaNs with Median Column value for each column\n",
    "\n",
    "#Pipeline for Logistic Regression with L1 Lasso Regularization\n",
    "pipe_lr_l1 = Pipeline([('imputer', impute),\n",
    "                       ('kbest', kbest),\n",
    "                      ('lr', logreg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for grid search cross validation \n",
    "parameters = {'kbest__k': [5, 10, 20,40, 60], #building models with 40, 60, 80, and 100 most significant variables\n",
    "                 'lr__C' : [0.01, 0.1, 1, 10, 100]} #tuning C for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search\n",
    "grid = GridSearchCV(pipe_lr_l1, parameters, cv = 5, scoring = 'roc_auc') #run grid on parameters, 5-fold cross validation, AUC is evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41min 21s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('kbest', SelectKBest(k=10, score_func=<function f_classif at 0x000001D758DF30D0>)), ('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'kbest__k': [5, 10, 20, 40, 60], 'lr__C': [0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#fit models\n",
    "grid.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model interpretations:<br><br>\n",
    "feature [86] is constant, meaning the value is constant for all visits and patients (and should be removed from model).<br><br>\n",
    "true_divide warning is likely due to feature[86], and is trying to divide by 0 variance...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:\n",
      "Pipeline(memory=None,\n",
      "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('kbest', SelectKBest(k=60, score_func=<function f_classif at 0x000001D758DF30D0>)), ('lr', LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=0, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "Logistic Regression step:\n",
      "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=0, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Logistic Regression coefficients:\n",
      "[[ 9.52683832e-04 -1.32666922e-01  1.19567914e-01  1.69686820e-01\n",
      "  -1.01592255e-02  9.30324824e-02  5.69836653e-02  9.69037316e-02\n",
      "  -4.59095858e-03  4.14169229e-02  1.53879735e-01  1.20989111e-01\n",
      "   1.07812544e-01  1.79547202e-01 -2.17536858e-02 -5.33387353e-02\n",
      "  -1.19502729e-02  4.84072800e-02  3.10457016e-02  4.65516862e-02\n",
      "   5.24860120e-02 -9.38383917e-02  6.43849016e-02 -3.27581190e-02\n",
      "  -1.47932913e-01 -6.28983605e-02 -9.05382957e-02 -2.22725596e-01\n",
      "   5.23189960e-02  7.46039815e-02  3.17239046e-02 -7.00114841e-02\n",
      "  -3.51952582e-02 -7.19563605e-02 -9.15547865e-02  7.57175911e-02\n",
      "  -9.77185174e-02 -4.15050614e-02 -4.83598814e-02 -1.63318886e-02\n",
      "  -1.19760171e-02 -2.42172796e-02 -1.48213854e-02 -3.07466560e-02\n",
      "   1.04659761e-02  2.95015449e-02 -9.26388478e-03  1.09772835e-02\n",
      "  -3.89540036e-01  2.08573298e-02  1.55618035e-02  3.26809137e-01\n",
      "   4.57919480e-01  1.44271967e+00 -8.39588277e-02  4.79936463e-01\n",
      "   2.80831084e-02  5.01410403e-01 -9.13027207e-02  2.62500613e-01]]\n",
      "Best Parameters: {'kbest__k': 60, 'lr__C': 100}\n",
      "Best cross validation score: 0.81\n"
     ]
    }
   ],
   "source": [
    "print(\"Best estimator:\\n{}\".format(grid.best_estimator_)) #prints best model and pipeline\n",
    "print(\"Logistic Regression step:\\n{}\".format(grid.best_estimator_.named_steps[\"lr\"])) #prints logistic regression step of pipeline\n",
    "print(\"Logistic Regression coefficients:\\n{}\".format(grid.best_estimator_.named_steps[\"lr\"].coef_)) #prints coefficients of best estimator\n",
    "print(\"Best Parameters: {}\".format(grid.best_params_)) #outputs best parameters settings\n",
    "print(\"Best cross validation score: {:.2f}\".format(grid.best_score_)) #best produced cross validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid scores on training set: \n",
      "0.753 (+/-0.001) for {'kbest__k': 5, 'lr__C': 0.01}\n",
      "0.753 (+/-0.001) for {'kbest__k': 5, 'lr__C': 0.1}\n",
      "0.753 (+/-0.001) for {'kbest__k': 5, 'lr__C': 1}\n",
      "0.753 (+/-0.001) for {'kbest__k': 5, 'lr__C': 10}\n",
      "0.753 (+/-0.001) for {'kbest__k': 5, 'lr__C': 100}\n",
      "0.766 (+/-0.001) for {'kbest__k': 10, 'lr__C': 0.01}\n",
      "0.766 (+/-0.001) for {'kbest__k': 10, 'lr__C': 0.1}\n",
      "0.766 (+/-0.001) for {'kbest__k': 10, 'lr__C': 1}\n",
      "0.766 (+/-0.001) for {'kbest__k': 10, 'lr__C': 10}\n",
      "0.766 (+/-0.001) for {'kbest__k': 10, 'lr__C': 100}\n",
      "0.787 (+/-0.001) for {'kbest__k': 20, 'lr__C': 0.01}\n",
      "0.787 (+/-0.001) for {'kbest__k': 20, 'lr__C': 0.1}\n",
      "0.787 (+/-0.001) for {'kbest__k': 20, 'lr__C': 1}\n",
      "0.787 (+/-0.001) for {'kbest__k': 20, 'lr__C': 10}\n",
      "0.787 (+/-0.001) for {'kbest__k': 20, 'lr__C': 100}\n",
      "0.807 (+/-0.000) for {'kbest__k': 40, 'lr__C': 0.01}\n",
      "0.808 (+/-0.000) for {'kbest__k': 40, 'lr__C': 0.1}\n",
      "0.808 (+/-0.000) for {'kbest__k': 40, 'lr__C': 1}\n",
      "0.808 (+/-0.000) for {'kbest__k': 40, 'lr__C': 10}\n",
      "0.808 (+/-0.000) for {'kbest__k': 40, 'lr__C': 100}\n",
      "0.813 (+/-0.001) for {'kbest__k': 60, 'lr__C': 0.01}\n",
      "0.814 (+/-0.000) for {'kbest__k': 60, 'lr__C': 0.1}\n",
      "0.814 (+/-0.000) for {'kbest__k': 60, 'lr__C': 1}\n",
      "0.814 (+/-0.000) for {'kbest__k': 60, 'lr__C': 10}\n",
      "0.814 (+/-0.000) for {'kbest__k': 60, 'lr__C': 100}\n",
      "Grid scores on Development set: \n",
      "0.753 (+/-0.003) for {'kbest__k': 5, 'lr__C': 0.01}\n",
      "0.753 (+/-0.003) for {'kbest__k': 5, 'lr__C': 0.1}\n",
      "0.753 (+/-0.003) for {'kbest__k': 5, 'lr__C': 1}\n",
      "0.753 (+/-0.003) for {'kbest__k': 5, 'lr__C': 10}\n",
      "0.753 (+/-0.003) for {'kbest__k': 5, 'lr__C': 100}\n",
      "0.765 (+/-0.002) for {'kbest__k': 10, 'lr__C': 0.01}\n",
      "0.766 (+/-0.003) for {'kbest__k': 10, 'lr__C': 0.1}\n",
      "0.766 (+/-0.003) for {'kbest__k': 10, 'lr__C': 1}\n",
      "0.766 (+/-0.003) for {'kbest__k': 10, 'lr__C': 10}\n",
      "0.766 (+/-0.003) for {'kbest__k': 10, 'lr__C': 100}\n",
      "0.787 (+/-0.002) for {'kbest__k': 20, 'lr__C': 0.01}\n",
      "0.787 (+/-0.002) for {'kbest__k': 20, 'lr__C': 0.1}\n",
      "0.787 (+/-0.002) for {'kbest__k': 20, 'lr__C': 1}\n",
      "0.787 (+/-0.002) for {'kbest__k': 20, 'lr__C': 10}\n",
      "0.787 (+/-0.002) for {'kbest__k': 20, 'lr__C': 100}\n",
      "0.807 (+/-0.002) for {'kbest__k': 40, 'lr__C': 0.01}\n",
      "0.807 (+/-0.002) for {'kbest__k': 40, 'lr__C': 0.1}\n",
      "0.807 (+/-0.002) for {'kbest__k': 40, 'lr__C': 1}\n",
      "0.807 (+/-0.002) for {'kbest__k': 40, 'lr__C': 10}\n",
      "0.807 (+/-0.002) for {'kbest__k': 40, 'lr__C': 100}\n",
      "0.813 (+/-0.002) for {'kbest__k': 60, 'lr__C': 0.01}\n",
      "0.814 (+/-0.002) for {'kbest__k': 60, 'lr__C': 0.1}\n",
      "0.814 (+/-0.002) for {'kbest__k': 60, 'lr__C': 1}\n",
      "0.814 (+/-0.002) for {'kbest__k': 60, 'lr__C': 10}\n",
      "0.814 (+/-0.002) for {'kbest__k': 60, 'lr__C': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "#AUC scores for each training set run\n",
    "print(\"Grid scores on training set: \")\n",
    "means = grid.cv_results_['mean_train_score']\n",
    "stds = grid.cv_results_['std_train_score']\n",
    "\n",
    "for mean, std, params in zip(means, stds, grid.cv_results_['params']): #note imbedded grid_l2 will need to be adjusted to correct object\n",
    "    print(\"%0.3f (+/-%0.3f) for %r\"\n",
    "         % (mean, std *2, params))\n",
    "    \n",
    "#AUC scores for each validation set run\n",
    "print(\"Grid scores on Development set: \")\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "\n",
    "for mean, std, params in zip(means, stds, grid.cv_results_['params']): #note imbedded grid_l2 will need to be adjusted to correct object\n",
    "    print(\"%0.3f (+/-%0.3f) for %r\"\n",
    "         % (mean, std *2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return variables of significance\n",
    "#start with 40\n",
    "fill_NaN = Imputer(missing_values=np.nan, strategy='median', axis=0)\n",
    "imputed_X_train = pd.DataFrame(fill_NaN.fit_transform(X_train))\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_train.index = X_train.index\n",
    "\n",
    "#display columns..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['gender', 'Anxiety', 'Depression', 'Drowsiness', 'Lack_of_Appetite',\n",
      "       'Nausea', 'Pain', 'Shortness_of_Breath', 'Tiredness', 'Wellbeing',\n",
      "       'LengthOfCare_days', '3_visit_max_anxiety', '3_visit_max_depression',\n",
      "       '3_visit_max_drowsiness', '3_visit_max_lackofappetite',\n",
      "       '3_visit_max_nausea', '3_visit_max_pain',\n",
      "       '3_visit_max_shortnessofbreath', '3_visit_max_tiredness',\n",
      "       '3_visit_max_wellbeing', '5_visit_max_anxiety',\n",
      "       '5_visit_max_depression', '5_visit_max_drowsiness',\n",
      "       '5_visit_max_lackofappetite', '5_visit_max_nausea', '5_visit_max_pain',\n",
      "       '5_visit_max_shortnessofbreath', '5_visit_max_tiredness',\n",
      "       '5_visit_max_wellbeing', '3_visit_mean_anxiety',\n",
      "       '3_visit_mean_depression', '3_visit_mean_drowsiness',\n",
      "       '3_visit_mean_lackofappetite', '3_visit_mean_nausea',\n",
      "       '3_visit_mean_pain', '3_visit_mean_shortnessofbreath',\n",
      "       '3_visit_mean_tiredness', '3_visit_mean_wellbeing',\n",
      "       '5_visit_mean_depression', '5_visit_mean_drowsiness',\n",
      "       '5_visit_mean_lackofappetite', '5_visit_mean_pain',\n",
      "       '5_visit_mean_shortnessofbreath', '5_visit_mean_tiredness',\n",
      "       '5_visit_mean_wellbeing', 'Anxiety_change', 'Depression_change',\n",
      "       'Drowsiness_change', 'LackofAppetite_change', 'Nausea_change',\n",
      "       'ShortnessofBreath_change', 'Tiredness_change', 'Wellbeing_change',\n",
      "       'Age', 'ESAS_visit_total', '3_visit_max_esas', '5_visit_max_esas',\n",
      "       '3_visit_mean_esas', '5_visit_mean_esas', 'ESAS_change',\n",
      "       'AdvanceDirective_Yes - Do Not Resuscitate (DNR)',\n",
      "       'AdvanceDirective_Yes - Full Code / Advanced Cardiac Life Support (ACLS)',\n",
      "       'ReferralType_Clinic or physician's office',\n",
      "       'ReferralType_Court/Law Enforcement',\n",
      "       'ReferralType_Information not available',\n",
      "       'ReferralType_Non-health care facility',\n",
      "       'ReferralType_Transfer from Home Health Agency',\n",
      "       'ReferralType_Transfer from Hospice',\n",
      "       'ReferralType_Transfer from SNF or ICF',\n",
      "       'ReferralType_Transfer from hospital', 'LevelofCare_Continuous (CHC)',\n",
      "       'LevelofCare_Inpatient (GIP)', 'LevelofCare_Routine',\n",
      "       'Race_American Indian or Alaskan Native', 'Race_Asian',\n",
      "       'Race_Black or African American', 'Race_Hispanic or Latino',\n",
      "       'Race_Native Hawaiian or Pacific Islander', 'Race_White',\n",
      "       'InsuranceType_Medicaid (HMO/Managed Care)',\n",
      "       'InsuranceType_Medicare Traditional', 'InsuranceType_Other',\n",
      "       'InsuranceType_Other Government',\n",
      "       'InsuranceType_Private HMO/Managed Care',\n",
      "       'InsuranceType_Private Insurance', 'InsuranceType_Private Pay',\n",
      "       'InsuranceType_Self-Pay',\n",
      "       'icd10_cluster_Certain conditions originating in the perinatal period',\n",
      "       'icd10_cluster_Certain infectious and parasitic diseases',\n",
      "       'icd10_cluster_Congenital malformations, deformations and chromosomal abnormalities',\n",
      "       'icd10_cluster_Diseases of the circulatory system',\n",
      "       'icd10_cluster_Diseases of the digestive system',\n",
      "       'icd10_cluster_Diseases of the genitourinary system',\n",
      "       'icd10_cluster_Diseases of the nervous system',\n",
      "       'icd10_cluster_Diseases of the respiratory system',\n",
      "       'icd10_cluster_Diseases of the skin and subcutaneous tissue',\n",
      "       'icd10_cluster_Endocrine, nutritional, and metabolic diseases',\n",
      "       'icd10_cluster_Injury, poisonining, and certain other consequences of external causes',\n",
      "       'icd10_cluster_Mental and behavioural disorders',\n",
      "       'icd10_cluster_Neoplasms'],\n",
      "      dtype='object')\n",
      "[1.39639765e+02 5.53526305e+02 2.26788422e+02 2.01742152e+04\n",
      " 2.72523017e+04 7.59494229e+01 9.72511216e+02 4.39515476e+03\n",
      " 1.90306416e+04 9.63413484e+03 4.91992871e+03 1.06550276e+03\n",
      " 3.61965269e+00 1.59568080e+04 1.91791931e+04 4.62453430e+01\n",
      " 2.21227066e+03 3.47081652e+03 1.41164880e+04 6.20866855e+03\n",
      " 4.33358136e+02 1.75625477e+01 9.73190319e+03 1.18132839e+04\n",
      " 3.90799786e+00 1.38156565e+03 1.79441042e+03 8.26320409e+03\n",
      " 3.59809945e+03 6.48067848e+01 3.48774803e+02 1.09940378e+04\n",
      " 1.63347417e+04 1.86669215e+01 5.03239221e+02 9.91341917e+02\n",
      " 1.23513887e+04 6.06793656e+03 2.12169711e+00 4.14300049e+02\n",
      " 6.06018694e+03 9.98998475e+03 4.42916930e-03 1.85285625e+02\n",
      " 2.44242586e+02 7.71492995e+03 3.43520195e+03 9.80567789e+00\n",
      " 6.85092525e+00 1.10275455e+03 1.16543256e+03 5.25006095e+00\n",
      " 2.02896973e+00 7.59932101e+02 8.20228337e+02 4.29440419e+02\n",
      " 1.92851110e+01 7.97722679e+03 4.36305815e+03 2.51169854e+03\n",
      " 3.45220034e+03 1.68414118e+03 4.46570174e+02 2.90981603e+00\n",
      " 1.59852397e+00 5.94693920e+01 4.78266673e+02 3.11369165e+02\n",
      " 1.27956769e+01 4.07761378e+01 4.39045540e+02 1.76352275e+02\n",
      " 1.13142359e+02 2.18056354e+01 3.04911190e+03 2.36509985e+02\n",
      " 8.36734446e+03 3.22009039e+00 6.11453484e+03 8.24524524e+00\n",
      " 1.03100386e+01 5.78477024e+01 5.66931788e+01 2.13696264e+01\n",
      " 1.30153506e+02 1.18519167e+01 3.12628784e-01 2.64716683e+00\n",
      " 6.23603098e-01 8.72748984e-02 4.31799374e+01 2.54120026e-02\n",
      " 1.63499256e+01 1.37349156e+01 1.50365065e+02 3.57169898e+01\n",
      " 6.33253435e+00 2.02023973e+01 1.16778903e+00 1.06985550e+00\n",
      " 3.12648215e+00 2.82066846e+00 6.39755578e+00 2.08746621e+02\n",
      " 1.35073853e+01 2.69275441e+00 1.89320130e+02 1.51079912e+01\n",
      " 3.03940909e+02 1.82027164e+00 5.09223710e+02 9.24890696e+01\n",
      " 2.15739260e+01 2.61549903e+01 2.70912523e+00 8.95320646e+01\n",
      " 3.80683045e+00 1.16502978e+03 2.45941179e+00]\n"
     ]
    }
   ],
   "source": [
    "#return variables of significance\n",
    "#Return 100 features used in best model\n",
    "selector = SelectKBest(f_classif, k = 100)\n",
    "selector.fit(imputed_X_train, y_train)\n",
    "features = imputed_X_train.columns[selector.get_support()]\n",
    "print(features)\n",
    "scores = selector.scores_ #saves scores of features\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression with L1 penalty and median missing data imputation** <br><br>\n",
    "- Best model selected used parameters K = 100 (100 variables included), and C = 1 for LogReg model. \n",
    "- AUC scores ranged from 0.807 (40 variables, C = 0.01-100) to 0.817"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with L2 Ridge Penalty, imputation = Median columnar value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression with L2 Regularization penalty\n",
    "kbest = SelectKBest(f_classif) #select best 'k' features\n",
    "logreg_l2 = LogisticRegression(penalty = 'l2', random_state = 0) #L2 Ridge Regularization\n",
    "impute = Imputer(missing_values = 'NaN', strategy = 'median', axis = 0) #impute missing values: replacing NaNs with Median Column value for each column\n",
    "\n",
    "#Pipeline for Logistic Regression with L2 'Ridge' Regularization\n",
    "pipe_lr_l2 = Pipeline([('imputer', impute),\n",
    "                       ('kbest', kbest),\n",
    "                      ('lr', logreg_l2)])\n",
    "#parameters for grid search cross validation \n",
    "parameters = {'kbest__k': [5, 10, 20, 40, 60], #building models with 40, 60, 80, and 100 most significant variables\n",
    "                 'lr__C' : [0.01, 0.1, 1, 10, 100]} #tuning C for logistic regression\n",
    "\n",
    "#grid search\n",
    "grid_l2 = GridSearchCV(pipe_lr_l2, parameters, cv = 5, scoring = 'roc_auc') #run grid on parameters, 5-fold cross validation, AUC is evaluation metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18min 43s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('kbest', SelectKBest(k=10, score_func=<function f_classif at 0x000001D758DF30D0>)), ('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'kbest__k': [5, 10, 20, 40, 60], 'lr__C': [0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#fit models\n",
    "grid_l2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:\n",
      "Pipeline(memory=None,\n",
      "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('kbest', SelectKBest(k=60, score_func=<function f_classif at 0x000001D758DF30D0>)), ('lr', LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "Logistic Regression step:\n",
      "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Logistic Regression coefficients:\n",
      "[[ 8.48669711e-04 -1.32466987e-01  1.19507325e-01  1.69695960e-01\n",
      "  -1.00971379e-02  9.30824113e-02  5.72070777e-02  9.71376716e-02\n",
      "  -4.58782396e-03  4.17210402e-02  1.53928058e-01  1.21826118e-01\n",
      "   1.08384728e-01  1.80426797e-01 -1.99880085e-02 -5.09853438e-02\n",
      "  -1.21642673e-02  4.82039093e-02  3.03597460e-02  4.60698330e-02\n",
      "   5.18031480e-02 -9.51375456e-02  6.23712201e-02 -3.36755036e-02\n",
      "  -1.47993345e-01 -6.39781338e-02 -9.15826694e-02 -2.24037525e-01\n",
      "   5.00185114e-02  7.14070015e-02  3.27160733e-02 -6.95660974e-02\n",
      "  -3.41448126e-02 -7.09312795e-02 -9.03962599e-02  7.73641760e-02\n",
      "  -9.49993332e-02 -4.14485331e-02 -4.83589586e-02 -1.63601174e-02\n",
      "  -1.21164429e-02 -2.43558984e-02 -1.49284195e-02 -3.26519862e-02\n",
      "   1.21043462e-02  3.19636408e-02 -1.15237265e-02  1.10542448e-02\n",
      "  -3.89076009e-01  2.08749417e-02  1.53772854e-02  3.26624179e-01\n",
      "   4.49645684e-01  1.43409840e+00 -9.15976060e-02  4.79915734e-01\n",
      "   2.78634769e-02  4.99456638e-01 -9.15865753e-02  2.62242617e-01]]\n",
      "Best Parameters: {'kbest__k': 60, 'lr__C': 10}\n",
      "Best cross validation score: 0.81\n"
     ]
    }
   ],
   "source": [
    "print(\"Best estimator:\\n{}\".format(grid_l2.best_estimator_)) #prints best model and pipeline\n",
    "print(\"Logistic Regression step:\\n{}\".format(grid_l2.best_estimator_.named_steps[\"lr\"])) #prints logistic regression step of pipeline\n",
    "print(\"Logistic Regression coefficients:\\n{}\".format(grid_l2.best_estimator_.named_steps[\"lr\"].coef_)) #prints coefficients of best estimator\n",
    "print(\"Best Parameters: {}\".format(grid_l2.best_params_)) #outputs best parameters settings\n",
    "print(\"Best cross validation score: {:.2f}\".format(grid_l2.best_score_)) #best produced cross validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid scores on Training set: \n",
      "0.753 (+/-0.001) for {'kbest__k': 5, 'lr__C': 0.01}\n",
      "0.753 (+/-0.001) for {'kbest__k': 5, 'lr__C': 0.1}\n",
      "0.753 (+/-0.001) for {'kbest__k': 5, 'lr__C': 1}\n",
      "0.753 (+/-0.001) for {'kbest__k': 5, 'lr__C': 10}\n",
      "0.753 (+/-0.001) for {'kbest__k': 5, 'lr__C': 100}\n",
      "0.766 (+/-0.001) for {'kbest__k': 10, 'lr__C': 0.01}\n",
      "0.766 (+/-0.001) for {'kbest__k': 10, 'lr__C': 0.1}\n",
      "0.766 (+/-0.001) for {'kbest__k': 10, 'lr__C': 1}\n",
      "0.766 (+/-0.001) for {'kbest__k': 10, 'lr__C': 10}\n",
      "0.766 (+/-0.001) for {'kbest__k': 10, 'lr__C': 100}\n",
      "0.787 (+/-0.001) for {'kbest__k': 20, 'lr__C': 0.01}\n",
      "0.787 (+/-0.001) for {'kbest__k': 20, 'lr__C': 0.1}\n",
      "0.787 (+/-0.001) for {'kbest__k': 20, 'lr__C': 1}\n",
      "0.787 (+/-0.001) for {'kbest__k': 20, 'lr__C': 10}\n",
      "0.787 (+/-0.001) for {'kbest__k': 20, 'lr__C': 100}\n",
      "0.807 (+/-0.000) for {'kbest__k': 40, 'lr__C': 0.01}\n",
      "0.808 (+/-0.000) for {'kbest__k': 40, 'lr__C': 0.1}\n",
      "0.808 (+/-0.000) for {'kbest__k': 40, 'lr__C': 1}\n",
      "0.808 (+/-0.000) for {'kbest__k': 40, 'lr__C': 10}\n",
      "0.808 (+/-0.000) for {'kbest__k': 40, 'lr__C': 100}\n",
      "0.814 (+/-0.001) for {'kbest__k': 60, 'lr__C': 0.01}\n",
      "0.814 (+/-0.001) for {'kbest__k': 60, 'lr__C': 0.1}\n",
      "0.814 (+/-0.000) for {'kbest__k': 60, 'lr__C': 1}\n",
      "0.814 (+/-0.000) for {'kbest__k': 60, 'lr__C': 10}\n",
      "0.814 (+/-0.000) for {'kbest__k': 60, 'lr__C': 100}\n",
      "Grid scores on Development set: \n",
      "0.753 (+/-0.003) for {'kbest__k': 5, 'lr__C': 0.01}\n",
      "0.753 (+/-0.003) for {'kbest__k': 5, 'lr__C': 0.1}\n",
      "0.753 (+/-0.003) for {'kbest__k': 5, 'lr__C': 1}\n",
      "0.753 (+/-0.003) for {'kbest__k': 5, 'lr__C': 10}\n",
      "0.753 (+/-0.003) for {'kbest__k': 5, 'lr__C': 100}\n",
      "0.765 (+/-0.003) for {'kbest__k': 10, 'lr__C': 0.01}\n",
      "0.766 (+/-0.003) for {'kbest__k': 10, 'lr__C': 0.1}\n",
      "0.766 (+/-0.003) for {'kbest__k': 10, 'lr__C': 1}\n",
      "0.766 (+/-0.003) for {'kbest__k': 10, 'lr__C': 10}\n",
      "0.766 (+/-0.003) for {'kbest__k': 10, 'lr__C': 100}\n",
      "0.786 (+/-0.002) for {'kbest__k': 20, 'lr__C': 0.01}\n",
      "0.787 (+/-0.002) for {'kbest__k': 20, 'lr__C': 0.1}\n",
      "0.787 (+/-0.002) for {'kbest__k': 20, 'lr__C': 1}\n",
      "0.787 (+/-0.002) for {'kbest__k': 20, 'lr__C': 10}\n",
      "0.787 (+/-0.002) for {'kbest__k': 20, 'lr__C': 100}\n",
      "0.807 (+/-0.002) for {'kbest__k': 40, 'lr__C': 0.01}\n",
      "0.807 (+/-0.002) for {'kbest__k': 40, 'lr__C': 0.1}\n",
      "0.807 (+/-0.002) for {'kbest__k': 40, 'lr__C': 1}\n",
      "0.807 (+/-0.002) for {'kbest__k': 40, 'lr__C': 10}\n",
      "0.807 (+/-0.002) for {'kbest__k': 40, 'lr__C': 100}\n",
      "0.813 (+/-0.002) for {'kbest__k': 60, 'lr__C': 0.01}\n",
      "0.814 (+/-0.002) for {'kbest__k': 60, 'lr__C': 0.1}\n",
      "0.814 (+/-0.002) for {'kbest__k': 60, 'lr__C': 1}\n",
      "0.814 (+/-0.002) for {'kbest__k': 60, 'lr__C': 10}\n",
      "0.814 (+/-0.002) for {'kbest__k': 60, 'lr__C': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "#AUC scores for each training set run\n",
    "print(\"Grid scores on Training set: \")\n",
    "means = grid_l2.cv_results_['mean_train_score']\n",
    "stds = grid_l2.cv_results_['std_train_score']\n",
    "\n",
    "for mean, std, params in zip(means, stds, grid_l2.cv_results_['params']): #note imbedded grid_l2 will need to be adjusted to correct object\n",
    "    print(\"%0.3f (+/-%0.3f) for %r\"\n",
    "         % (mean, std *2, params))\n",
    "\n",
    "#AUC scores for each validation set run\n",
    "print(\"Grid scores on Development set: \")\n",
    "means = grid_l2.cv_results_['mean_test_score']\n",
    "stds = grid_l2.cv_results_['std_test_score']\n",
    "\n",
    "for mean, std, params in zip(means, stds, grid_l2.cv_results_['params']): #note imbedded grid_l2 will need to be adjusted to correct object\n",
    "    print(\"%0.3f (+/-%0.3f) for %r\"\n",
    "         % (mean, std *2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Br>**Logistic Regression with L2 penalty and median missing data imputation**<br><br>\n",
    "- Best model selected used parameters K = 100 (100 variables included), and C = 10 for LogReg model. <br>\n",
    "- AUC scores ranged from 0.807 (40 variables, C = 0.01-100) to 0.817 \n",
    "- Interestingly, these outputs produced the same AUC across all cross validation trials as L1 model.\n",
    "    -Only difference was C = 1 for L1. \n",
    "    - Could be due to same random_state parameter?<br>\n",
    "    <br>\n",
    "    \n",
    "    Also interesting to note that adding 60 additional variables only improves AUC by ~1%. 100 variables is likely overfitting...\n",
    "    <br><Br>\n",
    "    Not sure why the training and test validation scores are exactly the same...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kNearest Neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 49min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# creating odd list of K for KNN\n",
    "neighbors = [1,3,5,7]\n",
    "\n",
    "# empty list that will hold cv scores\n",
    "cv_scores = []\n",
    "\n",
    "# perform 5-fold cross validation\n",
    "for k in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, imputed_X_train, y_train, cv=5, scoring='roc_auc') #using median imputed dataset for training\n",
    "    cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal number of neighbors is 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d71ab4b9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80191911 0.80626449 0.80662309 0.80448436 0.80376759]\n"
     ]
    }
   ],
   "source": [
    "# changing to misclassification error\n",
    "MSE = [1 - x for x in cv_scores]\n",
    "\n",
    "# determining best k\n",
    "optimal_k = neighbors[MSE.index(min(MSE))]\n",
    "print(\"The optimal number of neighbors is %d\" % optimal_k)\n",
    "\n",
    "# plot misclassification error vs k\n",
    "plt.plot(neighbors, MSE)\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('Misclassification Error')\n",
    "plt.show()\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision tree model\n",
    "tree = DecisionTreeClassifier(random_state = 0) #maximum depth of 5 leaves\n",
    "kbest = SelectKBest(f_classif) #select best 'k' features\n",
    "impute = Imputer(missing_values = 'NaN', strategy = 'median', axis = 0) #impute missing values: replacing NaNs with Median Column value for each column\n",
    "\n",
    "#Pipeline for Logistic Regression with L2 'Ridge' Regularization\n",
    "pipe_tree = Pipeline([('imputer', impute),\n",
    "                       ('kbest', kbest),\n",
    "                      ('dec_tree', tree)])\n",
    "#parameters for grid search cross validation \n",
    "parameters = {'kbest__k': [5, 10, 20, 40, 60], #building models with 40, 60, 80, and 100 most significant variables\n",
    "                 'dec_tree__max_depth' : [2,3,4,5]} #tuning tree depth for performance vs understanding\n",
    "\n",
    "#grid search\n",
    "grid_tree = GridSearchCV(pipe_tree, parameters, cv = 5, scoring = 'roc_auc') #run grid on parameters, 5-fold cross validation, AUC is evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('kbest', SelectKBest(k=10, score_func=<function f_classif at 0x000001D758DF30D0>)), ('dec_tree', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=N...         min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best'))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'kbest__k': [5, 10, 20, 40, 60], 'dec_tree__max_depth': [2, 3, 4, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#fit models\n",
    "grid_tree.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:\n",
      "Pipeline(memory=None,\n",
      "     steps=[('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('kbest', SelectKBest(k=60, score_func=<function f_classif at 0x000001D758DF30D0>)), ('dec_tree', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None...         min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
      "            splitter='best'))])\n",
      "Decision Tree step:\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
      "            splitter='best')\n",
      "Best Parameters: {'dec_tree__max_depth': 5, 'kbest__k': 60}\n",
      "Best cross validation score: 0.80\n"
     ]
    }
   ],
   "source": [
    "print(\"Best estimator:\\n{}\".format(grid_tree.best_estimator_)) #prints best model and pipeline\n",
    "print(\"Decision Tree step:\\n{}\".format(grid_tree.best_estimator_.named_steps[\"dec_tree\"])) #prints logistic regression step of pipeline\n",
    "#print(\"Logistic Regression coefficients:\\n{}\".format(grid_tree.best_estimator_.named_steps[\"dec_tree\"].coef_)) #prints coefficients of best estimator\n",
    "print(\"Best Parameters: {}\".format(grid_tree.best_params_)) #outputs best parameters settings\n",
    "print(\"Best cross validation score: {:.2f}\".format(grid_tree.best_score_)) #best produced cross validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid scores on Development set: \n",
      "0.731 (+/-0.001) for {'dec_tree__max_depth': 2, 'kbest__k': 5}\n",
      "0.731 (+/-0.001) for {'dec_tree__max_depth': 2, 'kbest__k': 10}\n",
      "0.731 (+/-0.001) for {'dec_tree__max_depth': 2, 'kbest__k': 20}\n",
      "0.731 (+/-0.001) for {'dec_tree__max_depth': 2, 'kbest__k': 40}\n",
      "0.731 (+/-0.001) for {'dec_tree__max_depth': 2, 'kbest__k': 60}\n",
      "0.766 (+/-0.001) for {'dec_tree__max_depth': 3, 'kbest__k': 5}\n",
      "0.766 (+/-0.001) for {'dec_tree__max_depth': 3, 'kbest__k': 10}\n",
      "0.751 (+/-0.001) for {'dec_tree__max_depth': 3, 'kbest__k': 20}\n",
      "0.752 (+/-0.001) for {'dec_tree__max_depth': 3, 'kbest__k': 40}\n",
      "0.752 (+/-0.001) for {'dec_tree__max_depth': 3, 'kbest__k': 60}\n",
      "0.783 (+/-0.001) for {'dec_tree__max_depth': 4, 'kbest__k': 5}\n",
      "0.783 (+/-0.001) for {'dec_tree__max_depth': 4, 'kbest__k': 10}\n",
      "0.780 (+/-0.001) for {'dec_tree__max_depth': 4, 'kbest__k': 20}\n",
      "0.781 (+/-0.001) for {'dec_tree__max_depth': 4, 'kbest__k': 40}\n",
      "0.781 (+/-0.001) for {'dec_tree__max_depth': 4, 'kbest__k': 60}\n",
      "0.795 (+/-0.001) for {'dec_tree__max_depth': 5, 'kbest__k': 5}\n",
      "0.795 (+/-0.001) for {'dec_tree__max_depth': 5, 'kbest__k': 10}\n",
      "0.795 (+/-0.001) for {'dec_tree__max_depth': 5, 'kbest__k': 20}\n",
      "0.798 (+/-0.001) for {'dec_tree__max_depth': 5, 'kbest__k': 40}\n",
      "0.798 (+/-0.001) for {'dec_tree__max_depth': 5, 'kbest__k': 60}\n",
      "Grid scores on Development set: \n",
      "0.731 (+/-0.005) for {'dec_tree__max_depth': 2, 'kbest__k': 5}\n",
      "0.731 (+/-0.005) for {'dec_tree__max_depth': 2, 'kbest__k': 10}\n",
      "0.731 (+/-0.005) for {'dec_tree__max_depth': 2, 'kbest__k': 20}\n",
      "0.731 (+/-0.005) for {'dec_tree__max_depth': 2, 'kbest__k': 40}\n",
      "0.731 (+/-0.005) for {'dec_tree__max_depth': 2, 'kbest__k': 60}\n",
      "0.765 (+/-0.004) for {'dec_tree__max_depth': 3, 'kbest__k': 5}\n",
      "0.765 (+/-0.004) for {'dec_tree__max_depth': 3, 'kbest__k': 10}\n",
      "0.750 (+/-0.004) for {'dec_tree__max_depth': 3, 'kbest__k': 20}\n",
      "0.752 (+/-0.003) for {'dec_tree__max_depth': 3, 'kbest__k': 40}\n",
      "0.752 (+/-0.003) for {'dec_tree__max_depth': 3, 'kbest__k': 60}\n",
      "0.782 (+/-0.004) for {'dec_tree__max_depth': 4, 'kbest__k': 5}\n",
      "0.783 (+/-0.004) for {'dec_tree__max_depth': 4, 'kbest__k': 10}\n",
      "0.779 (+/-0.003) for {'dec_tree__max_depth': 4, 'kbest__k': 20}\n",
      "0.780 (+/-0.003) for {'dec_tree__max_depth': 4, 'kbest__k': 40}\n",
      "0.780 (+/-0.003) for {'dec_tree__max_depth': 4, 'kbest__k': 60}\n",
      "0.794 (+/-0.003) for {'dec_tree__max_depth': 5, 'kbest__k': 5}\n",
      "0.794 (+/-0.003) for {'dec_tree__max_depth': 5, 'kbest__k': 10}\n",
      "0.794 (+/-0.004) for {'dec_tree__max_depth': 5, 'kbest__k': 20}\n",
      "0.796 (+/-0.004) for {'dec_tree__max_depth': 5, 'kbest__k': 40}\n",
      "0.796 (+/-0.004) for {'dec_tree__max_depth': 5, 'kbest__k': 60}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "#AUC scores for each training set run\n",
    "print(\"Grid scores on Development set: \")\n",
    "means = grid_tree.cv_results_['mean_train_score']\n",
    "stds = grid_tree.cv_results_['std_train_score']\n",
    "\n",
    "for mean, std, params in zip(means, stds, grid_tree.cv_results_['params']): #note imbedded grid_l2 will need to be adjusted to correct object\n",
    "    print(\"%0.3f (+/-%0.3f) for %r\"\n",
    "         % (mean, std *2, params))\n",
    "\n",
    "#AUC scores for each validation set run\n",
    "print(\"Grid scores on Development set: \")\n",
    "means = grid_tree.cv_results_['mean_test_score']\n",
    "stds = grid_tree.cv_results_['std_test_score']\n",
    "\n",
    "for mean, std, params in zip(means, stds, grid_tree.cv_results_['params']): #note imbedded grid_l2 will need to be adjusted to correct object\n",
    "    print(\"%0.3f (+/-%0.3f) for %r\"\n",
    "         % (mean, std *2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data: '0' fills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can't pipeline is_na = 0, so will need to do manually for X_train, y_train\n",
    "X_train_zero = X_train.fillna(0)\n",
    "y_train_zero = y_train.fillna(0)\n",
    "#use these objects for cross validation, and remove 'impute' methods from any pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with L1 Lasso Penalty, missing data filled with '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "kbest = SelectKBest(f_classif) #select best 'k' features\n",
    "logreg = LogisticRegression(penalty = 'l1', random_state = 0) #L1 Regularization\n",
    "\n",
    "#Pipeline for Logistic Regression with L1 Lasso Regularization\n",
    "pipe_lr_l1 = Pipeline([('kbest', kbest),\n",
    "                      ('lr', logreg)])\n",
    "\n",
    "#parameters for grid search cross validation \n",
    "parameters = {'kbest__k': [5, 10, 20,40, 60, 100], #building models with 40, 60, 80, and 100 most significant variables\n",
    "                 'lr__C' : [0.01, 0.1, 1, 10, 100]} #tuning C for logistic regression\n",
    "\n",
    "#grid search\n",
    "grid_l1 = GridSearchCV(pipe_lr_l1, parameters, cv = 5, scoring = 'roc_auc') #run grid on parameters, 5-fold cross validation, AUC is evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14min 10s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('kbest', SelectKBest(k=10, score_func=<function f_classif at 0x000001D758DF30D0>)), ('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'kbest__k': [5, 10, 20, 40, 60, 100], 'lr__C': [0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#fit model on missing data = 0 training set\n",
    "grid_l1.fit(X_train_zero,y_train_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:\n",
      "Pipeline(memory=None,\n",
      "     steps=[('kbest', SelectKBest(k=100, score_func=<function f_classif at 0x000001D758DF30D0>)), ('lr', LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=0, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "Logistic Regression step:\n",
      "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=0, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Logistic Regression coefficients:\n",
      "[[-1.32341561e-01  1.81731940e-02 -1.77457827e-01  1.50869695e-01\n",
      "   1.24549568e-01 -4.74059342e-02  1.60517218e-02  1.06199762e-01\n",
      "  -4.33025264e-02 -3.77252291e-03 -5.21727590e-03 -4.17332787e-02\n",
      "   1.27063412e-02 -2.98212161e-02 -3.68404459e-02  8.42054997e-02\n",
      "   1.30212743e-01 -9.09501512e-02  5.34454249e-02  4.94546351e-02\n",
      "  -9.16636218e-02  3.52406894e-02  2.66767558e-02 -2.93302833e-03\n",
      "   1.94344551e-02  1.00966345e-01 -9.58458659e-02  6.10836066e-02\n",
      "   1.71475228e-02  7.21619544e-02  6.08925496e-03  5.37703182e-02\n",
      "   9.32983884e-02 -9.77035460e-02 -1.47823717e-01  7.45972008e-02\n",
      "  -4.82629346e-02 -8.63276660e-02  1.60005832e-01 -5.84965033e-02\n",
      "  -3.18998008e-02 -1.59884819e-01  8.15260279e-02 -7.55108746e-02\n",
      "  -1.15155956e-02  5.04802575e-02 -3.91975148e-02 -7.55433282e-03\n",
      "   6.38227500e-03 -2.09166304e-02  4.30303695e-02  2.67881239e-02\n",
      "   4.05557073e-03 -9.58272321e-03  7.60159280e-03 -5.06489197e-03\n",
      "  -9.00405529e-03 -2.40431086e-05  1.83881343e-02 -4.13139752e-01\n",
      "  -1.07588064e+00  3.58412661e+00 -1.13748521e+00 -1.08752194e+00\n",
      "  -1.20850404e+00 -1.32954555e+00 -9.46533738e-01 -7.56874051e-01\n",
      "  -2.94824050e-01  8.60818388e-01 -6.44348273e-01 -8.25904430e-01\n",
      "   1.38966828e-01 -1.13445029e-01 -3.66699825e-01 -2.63870005e-02\n",
      "   2.41747919e-01  3.64142431e-02 -3.53479201e-02  1.20747039e-01\n",
      "   3.97979367e-01 -9.41268374e-01  3.62415692e-01  1.79256910e-01\n",
      "   6.65878610e-01  6.44925491e-01 -5.17037446e+00  4.26806304e+00\n",
      "   5.56022530e-01 -5.20403872e-01  7.54099745e-03  3.21845028e-01\n",
      "   5.50861492e-01 -7.04896544e-02 -5.48950658e-02 -5.87816594e-01\n",
      "  -1.26199166e-01  6.34629006e-01 -4.19549956e-02  3.30579303e-01]]\n",
      "Best Parameters: {'kbest__k': 100, 'lr__C': 10}\n",
      "Best cross validation score: 0.77\n"
     ]
    }
   ],
   "source": [
    "print(\"Best estimator:\\n{}\".format(grid_l1.best_estimator_)) #prints best model and pipeline\n",
    "print(\"Logistic Regression step:\\n{}\".format(grid_l1.best_estimator_.named_steps[\"lr\"])) #prints logistic regression step of pipeline\n",
    "print(\"Logistic Regression coefficients:\\n{}\".format(grid_l1.best_estimator_.named_steps[\"lr\"].coef_)) #prints coefficients of best estimator\n",
    "print(\"Best Parameters: {}\".format(grid_l1.best_params_)) #outputs best parameters settings\n",
    "print(\"Best cross validation score: {:.2f}\".format(grid_l1.best_score_)) #best produced cross validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid scores on training set: \n",
      "0.703 (+/-0.001) for {'kbest__k': 5, 'lr__C': 0.01}\n",
      "0.703 (+/-0.001) for {'kbest__k': 5, 'lr__C': 0.1}\n",
      "0.703 (+/-0.001) for {'kbest__k': 5, 'lr__C': 1}\n",
      "0.703 (+/-0.001) for {'kbest__k': 5, 'lr__C': 10}\n",
      "0.703 (+/-0.001) for {'kbest__k': 5, 'lr__C': 100}\n",
      "0.732 (+/-0.001) for {'kbest__k': 10, 'lr__C': 0.01}\n",
      "0.732 (+/-0.001) for {'kbest__k': 10, 'lr__C': 0.1}\n",
      "0.732 (+/-0.001) for {'kbest__k': 10, 'lr__C': 1}\n",
      "0.732 (+/-0.001) for {'kbest__k': 10, 'lr__C': 10}\n",
      "0.732 (+/-0.001) for {'kbest__k': 10, 'lr__C': 100}\n",
      "0.749 (+/-0.004) for {'kbest__k': 20, 'lr__C': 0.01}\n",
      "0.749 (+/-0.005) for {'kbest__k': 20, 'lr__C': 0.1}\n",
      "0.749 (+/-0.005) for {'kbest__k': 20, 'lr__C': 1}\n",
      "0.749 (+/-0.005) for {'kbest__k': 20, 'lr__C': 10}\n",
      "0.749 (+/-0.005) for {'kbest__k': 20, 'lr__C': 100}\n",
      "0.759 (+/-0.001) for {'kbest__k': 40, 'lr__C': 0.01}\n",
      "0.759 (+/-0.001) for {'kbest__k': 40, 'lr__C': 0.1}\n",
      "0.759 (+/-0.001) for {'kbest__k': 40, 'lr__C': 1}\n",
      "0.759 (+/-0.001) for {'kbest__k': 40, 'lr__C': 10}\n",
      "0.759 (+/-0.001) for {'kbest__k': 40, 'lr__C': 100}\n",
      "0.765 (+/-0.001) for {'kbest__k': 60, 'lr__C': 0.01}\n",
      "0.766 (+/-0.001) for {'kbest__k': 60, 'lr__C': 0.1}\n",
      "0.766 (+/-0.001) for {'kbest__k': 60, 'lr__C': 1}\n",
      "0.766 (+/-0.001) for {'kbest__k': 60, 'lr__C': 10}\n",
      "0.766 (+/-0.001) for {'kbest__k': 60, 'lr__C': 100}\n",
      "0.767 (+/-0.001) for {'kbest__k': 100, 'lr__C': 0.01}\n",
      "0.769 (+/-0.001) for {'kbest__k': 100, 'lr__C': 0.1}\n",
      "0.769 (+/-0.001) for {'kbest__k': 100, 'lr__C': 1}\n",
      "0.769 (+/-0.001) for {'kbest__k': 100, 'lr__C': 10}\n",
      "0.769 (+/-0.001) for {'kbest__k': 100, 'lr__C': 100}\n",
      "Grid scores on Development set: \n",
      "0.703 (+/-0.003) for {'kbest__k': 5, 'lr__C': 0.01}\n",
      "0.703 (+/-0.003) for {'kbest__k': 5, 'lr__C': 0.1}\n",
      "0.703 (+/-0.003) for {'kbest__k': 5, 'lr__C': 1}\n",
      "0.703 (+/-0.003) for {'kbest__k': 5, 'lr__C': 10}\n",
      "0.703 (+/-0.003) for {'kbest__k': 5, 'lr__C': 100}\n",
      "0.732 (+/-0.005) for {'kbest__k': 10, 'lr__C': 0.01}\n",
      "0.732 (+/-0.005) for {'kbest__k': 10, 'lr__C': 0.1}\n",
      "0.732 (+/-0.005) for {'kbest__k': 10, 'lr__C': 1}\n",
      "0.732 (+/-0.005) for {'kbest__k': 10, 'lr__C': 10}\n",
      "0.732 (+/-0.005) for {'kbest__k': 10, 'lr__C': 100}\n",
      "0.748 (+/-0.007) for {'kbest__k': 20, 'lr__C': 0.01}\n",
      "0.748 (+/-0.007) for {'kbest__k': 20, 'lr__C': 0.1}\n",
      "0.748 (+/-0.007) for {'kbest__k': 20, 'lr__C': 1}\n",
      "0.748 (+/-0.007) for {'kbest__k': 20, 'lr__C': 10}\n",
      "0.748 (+/-0.007) for {'kbest__k': 20, 'lr__C': 100}\n",
      "0.758 (+/-0.004) for {'kbest__k': 40, 'lr__C': 0.01}\n",
      "0.758 (+/-0.003) for {'kbest__k': 40, 'lr__C': 0.1}\n",
      "0.758 (+/-0.003) for {'kbest__k': 40, 'lr__C': 1}\n",
      "0.758 (+/-0.003) for {'kbest__k': 40, 'lr__C': 10}\n",
      "0.758 (+/-0.003) for {'kbest__k': 40, 'lr__C': 100}\n",
      "0.764 (+/-0.004) for {'kbest__k': 60, 'lr__C': 0.01}\n",
      "0.765 (+/-0.003) for {'kbest__k': 60, 'lr__C': 0.1}\n",
      "0.765 (+/-0.003) for {'kbest__k': 60, 'lr__C': 1}\n",
      "0.765 (+/-0.003) for {'kbest__k': 60, 'lr__C': 10}\n",
      "0.765 (+/-0.003) for {'kbest__k': 60, 'lr__C': 100}\n",
      "0.766 (+/-0.005) for {'kbest__k': 100, 'lr__C': 0.01}\n",
      "0.768 (+/-0.004) for {'kbest__k': 100, 'lr__C': 0.1}\n",
      "0.768 (+/-0.004) for {'kbest__k': 100, 'lr__C': 1}\n",
      "0.768 (+/-0.004) for {'kbest__k': 100, 'lr__C': 10}\n",
      "0.768 (+/-0.004) for {'kbest__k': 100, 'lr__C': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "#AUC scores for each training set run\n",
    "print(\"Grid scores on training set: \")\n",
    "means = grid_l1.cv_results_['mean_train_score']\n",
    "stds = grid_l1.cv_results_['std_train_score']\n",
    "\n",
    "for mean, std, params in zip(means, stds, grid_l1.cv_results_['params']): \n",
    "    print(\"%0.3f (+/-%0.3f) for %r\"\n",
    "         % (mean, std *2, params))\n",
    "    \n",
    "#AUC scores for each validation set run\n",
    "print(\"Grid scores on Development set: \")\n",
    "means = grid_l1.cv_results_['mean_test_score']\n",
    "stds = grid_l1.cv_results_['std_test_score']\n",
    "\n",
    "for mean, std, params in zip(means, stds, grid_l1.cv_results_['params']): \n",
    "    print(\"%0.3f (+/-%0.3f) for %r\"\n",
    "         % (mean, std *2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with L2 Ridge Penalty, missing data = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression with L2 Regularization penalty\n",
    "kbest = SelectKBest(f_classif) #select best 'k' features\n",
    "logreg_l2 = LogisticRegression(penalty = 'l2', random_state = 0) #L2 Ridge Regularization\n",
    "\n",
    "#Pipeline for Logistic Regression with L2 'Ridge' Regularization\n",
    "pipe_lr_l2 = Pipeline([('kbest', kbest),\n",
    "                      ('lr', logreg_l2)])\n",
    "#parameters for grid search cross validation \n",
    "parameters = {'kbest__k': [5, 10, 20, 40, 60, 100], #building models with 40, 60, 80, and 100 most significant variables\n",
    "                 'lr__C' : [0.01, 0.1, 1, 10, 100]} #tuning C for logistic regression\n",
    "\n",
    "#grid search\n",
    "grid_l2 = GridSearchCV(pipe_lr_l2, parameters, cv = 5, scoring = 'roc_auc') #run grid on parameters, 5-fold cross validation, AUC is evaluation metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17min 24s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('kbest', SelectKBest(k=10, score_func=<function f_classif at 0x000001D758DF30D0>)), ('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'kbest__k': [5, 10, 20, 40, 60, 100], 'lr__C': [0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#fit models\n",
    "grid_l2.fit(X_train_zero,y_train_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:\n",
      "Pipeline(memory=None,\n",
      "     steps=[('kbest', SelectKBest(k=100, score_func=<function f_classif at 0x000001D758DF30D0>)), ('lr', LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n",
      "Logistic Regression step:\n",
      "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Logistic Regression coefficients:\n",
      "[[-1.34150004e-01  1.75154671e-02 -1.76953989e-01  1.50851119e-01\n",
      "   1.24340841e-01 -4.78980494e-02  1.61794858e-02  1.06309585e-01\n",
      "  -4.34248082e-02 -3.70721713e-03 -5.23990210e-03 -4.03387263e-02\n",
      "   1.37042960e-02 -3.06641391e-02 -3.67139997e-02  8.41951754e-02\n",
      "   1.31216009e-01 -9.08878396e-02  5.45945337e-02  4.93525174e-02\n",
      "  -9.24993470e-02  3.44133318e-02  2.70068641e-02 -3.08368349e-03\n",
      "   1.95328008e-02  1.00715617e-01 -9.61426066e-02  5.97564240e-02\n",
      "   1.84524877e-02  6.88374953e-02  4.67231229e-03  5.44907613e-02\n",
      "   9.34477475e-02 -9.79640656e-02 -1.49211875e-01  7.47878248e-02\n",
      "  -4.98868459e-02 -8.67591811e-02  1.61902438e-01 -5.70017804e-02\n",
      "  -3.18830254e-02 -1.59536321e-01  8.15611691e-02 -7.38326717e-02\n",
      "  -1.09461862e-02  4.99129729e-02 -3.92147825e-02 -7.57727715e-03\n",
      "   6.56576186e-03 -2.10373890e-02  4.30153505e-02  2.67641089e-02\n",
      "   3.98416613e-03 -1.07439528e-02  8.63108047e-03 -3.60704905e-03\n",
      "  -1.03512988e-02  5.42896625e-05  1.62828524e-02 -4.20108728e-01\n",
      "  -1.89665075e-01  1.22321620e-01 -2.59383819e-01 -2.00799629e-01\n",
      "  -3.27267615e-01 -4.34353208e-01 -6.21923604e-02  1.29505583e-01\n",
      "  -3.84518020e-01  7.65927977e-01 -7.41089026e-01 -9.21804932e-01\n",
      "   1.42989828e-01 -1.14432858e-01 -3.66041691e-01 -2.66170955e-02\n",
      "   2.49326160e-01  3.59194773e-02 -3.17914965e-02  1.21008020e-01\n",
      "   4.03741594e-01 -3.15119565e-01  3.55128008e-01  1.84768474e-01\n",
      "   1.61592982e-01  5.89232468e-01 -2.43626019e-01  6.92671751e-02\n",
      "   5.61893186e-01 -3.38911534e-01  1.20684197e-02  3.20463058e-01\n",
      "   5.60236684e-01 -6.59333382e-02 -5.07098842e-02 -3.97744053e-01\n",
      "  -1.19896111e-01  6.42736660e-01 -5.03330356e-02  3.33622285e-01]]\n",
      "Best Parameters: {'kbest__k': 100, 'lr__C': 100}\n",
      "Best cross validation score: 0.77\n"
     ]
    }
   ],
   "source": [
    "print(\"Best estimator:\\n{}\".format(grid_l2.best_estimator_)) #prints best model and pipeline\n",
    "print(\"Logistic Regression step:\\n{}\".format(grid_l2.best_estimator_.named_steps[\"lr\"])) #prints logistic regression step of pipeline\n",
    "print(\"Logistic Regression coefficients:\\n{}\".format(grid_l2.best_estimator_.named_steps[\"lr\"].coef_)) #prints coefficients of best estimator\n",
    "print(\"Best Parameters: {}\".format(grid_l2.best_params_)) #outputs best parameters settings\n",
    "print(\"Best cross validation score: {:.2f}\".format(grid_l2.best_score_)) #best produced cross validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid scores on training set: \n",
      "0.703 (+/-0.001) for {'kbest__k': 5, 'lr__C': 0.01}\n",
      "0.703 (+/-0.001) for {'kbest__k': 5, 'lr__C': 0.1}\n",
      "0.703 (+/-0.001) for {'kbest__k': 5, 'lr__C': 1}\n",
      "0.703 (+/-0.001) for {'kbest__k': 5, 'lr__C': 10}\n",
      "0.703 (+/-0.001) for {'kbest__k': 5, 'lr__C': 100}\n",
      "0.732 (+/-0.001) for {'kbest__k': 10, 'lr__C': 0.01}\n",
      "0.732 (+/-0.001) for {'kbest__k': 10, 'lr__C': 0.1}\n",
      "0.732 (+/-0.001) for {'kbest__k': 10, 'lr__C': 1}\n",
      "0.732 (+/-0.001) for {'kbest__k': 10, 'lr__C': 10}\n",
      "0.732 (+/-0.001) for {'kbest__k': 10, 'lr__C': 100}\n",
      "0.749 (+/-0.005) for {'kbest__k': 20, 'lr__C': 0.01}\n",
      "0.749 (+/-0.005) for {'kbest__k': 20, 'lr__C': 0.1}\n",
      "0.749 (+/-0.005) for {'kbest__k': 20, 'lr__C': 1}\n",
      "0.749 (+/-0.005) for {'kbest__k': 20, 'lr__C': 10}\n",
      "0.749 (+/-0.005) for {'kbest__k': 20, 'lr__C': 100}\n",
      "0.759 (+/-0.001) for {'kbest__k': 40, 'lr__C': 0.01}\n",
      "0.759 (+/-0.001) for {'kbest__k': 40, 'lr__C': 0.1}\n",
      "0.759 (+/-0.001) for {'kbest__k': 40, 'lr__C': 1}\n",
      "0.759 (+/-0.001) for {'kbest__k': 40, 'lr__C': 10}\n",
      "0.759 (+/-0.001) for {'kbest__k': 40, 'lr__C': 100}\n",
      "0.766 (+/-0.001) for {'kbest__k': 60, 'lr__C': 0.01}\n",
      "0.766 (+/-0.001) for {'kbest__k': 60, 'lr__C': 0.1}\n",
      "0.766 (+/-0.001) for {'kbest__k': 60, 'lr__C': 1}\n",
      "0.766 (+/-0.001) for {'kbest__k': 60, 'lr__C': 10}\n",
      "0.766 (+/-0.001) for {'kbest__k': 60, 'lr__C': 100}\n",
      "0.768 (+/-0.001) for {'kbest__k': 100, 'lr__C': 0.01}\n",
      "0.769 (+/-0.001) for {'kbest__k': 100, 'lr__C': 0.1}\n",
      "0.769 (+/-0.001) for {'kbest__k': 100, 'lr__C': 1}\n",
      "0.769 (+/-0.001) for {'kbest__k': 100, 'lr__C': 10}\n",
      "0.769 (+/-0.001) for {'kbest__k': 100, 'lr__C': 100}\n",
      "Grid scores on Development set: \n",
      "0.703 (+/-0.003) for {'kbest__k': 5, 'lr__C': 0.01}\n",
      "0.703 (+/-0.003) for {'kbest__k': 5, 'lr__C': 0.1}\n",
      "0.703 (+/-0.003) for {'kbest__k': 5, 'lr__C': 1}\n",
      "0.703 (+/-0.003) for {'kbest__k': 5, 'lr__C': 10}\n",
      "0.703 (+/-0.003) for {'kbest__k': 5, 'lr__C': 100}\n",
      "0.732 (+/-0.005) for {'kbest__k': 10, 'lr__C': 0.01}\n",
      "0.732 (+/-0.005) for {'kbest__k': 10, 'lr__C': 0.1}\n",
      "0.732 (+/-0.004) for {'kbest__k': 10, 'lr__C': 1}\n",
      "0.732 (+/-0.004) for {'kbest__k': 10, 'lr__C': 10}\n",
      "0.732 (+/-0.004) for {'kbest__k': 10, 'lr__C': 100}\n",
      "0.749 (+/-0.007) for {'kbest__k': 20, 'lr__C': 0.01}\n",
      "0.748 (+/-0.007) for {'kbest__k': 20, 'lr__C': 0.1}\n",
      "0.748 (+/-0.007) for {'kbest__k': 20, 'lr__C': 1}\n",
      "0.748 (+/-0.007) for {'kbest__k': 20, 'lr__C': 10}\n",
      "0.748 (+/-0.007) for {'kbest__k': 20, 'lr__C': 100}\n",
      "0.759 (+/-0.004) for {'kbest__k': 40, 'lr__C': 0.01}\n",
      "0.758 (+/-0.003) for {'kbest__k': 40, 'lr__C': 0.1}\n",
      "0.758 (+/-0.003) for {'kbest__k': 40, 'lr__C': 1}\n",
      "0.758 (+/-0.003) for {'kbest__k': 40, 'lr__C': 10}\n",
      "0.758 (+/-0.003) for {'kbest__k': 40, 'lr__C': 100}\n",
      "0.765 (+/-0.003) for {'kbest__k': 60, 'lr__C': 0.01}\n",
      "0.765 (+/-0.003) for {'kbest__k': 60, 'lr__C': 0.1}\n",
      "0.765 (+/-0.003) for {'kbest__k': 60, 'lr__C': 1}\n",
      "0.765 (+/-0.003) for {'kbest__k': 60, 'lr__C': 10}\n",
      "0.765 (+/-0.003) for {'kbest__k': 60, 'lr__C': 100}\n",
      "0.768 (+/-0.005) for {'kbest__k': 100, 'lr__C': 0.01}\n",
      "0.768 (+/-0.004) for {'kbest__k': 100, 'lr__C': 0.1}\n",
      "0.768 (+/-0.004) for {'kbest__k': 100, 'lr__C': 1}\n",
      "0.768 (+/-0.004) for {'kbest__k': 100, 'lr__C': 10}\n",
      "0.768 (+/-0.004) for {'kbest__k': 100, 'lr__C': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "#AUC scores for each training set run\n",
    "print(\"Grid scores on training set: \")\n",
    "means = grid_l2.cv_results_['mean_train_score']\n",
    "stds = grid_l2.cv_results_['std_train_score']\n",
    "\n",
    "for mean, std, params in zip(means, stds, grid_l2.cv_results_['params']): #note imbedded grid_l2 will need to be adjusted to correct object\n",
    "    print(\"%0.3f (+/-%0.3f) for %r\"\n",
    "         % (mean, std *2, params))\n",
    "\n",
    "#AUC scores for each validation set run\n",
    "print(\"Grid scores on Development set: \")\n",
    "means = grid_l2.cv_results_['mean_test_score']\n",
    "stds = grid_l2.cv_results_['std_test_score']\n",
    "\n",
    "for mean, std, params in zip(means, stds, grid_l2.cv_results_['params']): #note imbedded grid_l2 will need to be adjusted to correct object\n",
    "    print(\"%0.3f (+/-%0.3f) for %r\"\n",
    "         % (mean, std *2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kNearest Neighbors<br><br>\n",
    "Missing data = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# creating odd list of K for KNN\n",
    "neighbors = [1,3,5,7,9,11]\n",
    "\n",
    "# empty list that will hold cv scores\n",
    "cv_scores = []\n",
    "\n",
    "# perform 5-fold cross validation\n",
    "for k in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train_zero, y_train_zero, cv=5, scoring='roc_auc') #using median imputed dataset for training\n",
    "    cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal number of neighbors is 11\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d71af2e630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80608308 0.8080598  0.80982286 0.80678216 0.80617784]\n"
     ]
    }
   ],
   "source": [
    "# changing to misclassification error\n",
    "MSE = [1 - x for x in cv_scores]\n",
    "\n",
    "# determining best k\n",
    "optimal_k = neighbors[MSE.index(min(MSE))]\n",
    "print(\"The optimal number of neighbors is %d\" % optimal_k)\n",
    "\n",
    "# plot misclassification error vs k\n",
    "plt.plot(neighbors, MSE)\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('Misclassification Error')\n",
    "plt.show()\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree with 0 as missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision tree model\n",
    "tree = DecisionTreeClassifier(random_state = 0) #maximum depth of 5 leaves\n",
    "kbest = SelectKBest(f_classif) #select best 'k' features\n",
    "\n",
    "#Pipeline for Logistic Regression with L2 'Ridge' Regularization\n",
    "pipe_tree = Pipeline([('kbest', kbest),\n",
    "                      ('dec_tree', tree)])\n",
    "#parameters for grid search cross validation \n",
    "parameters = {'kbest__k': [5, 10, 20, 40, 60, 100], #building models with 40, 60, 80, and 100 most significant variables\n",
    "                 'dec_tree__max_depth' : [2,3,4,5]} #tuning tree depth for performance vs understanding\n",
    "\n",
    "#grid search\n",
    "grid_tree = GridSearchCV(pipe_tree, parameters, cv = 5, scoring = 'roc_auc') #run grid on parameters, 5-fold cross validation, AUC is evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:113: UserWarning: Features [86] are constant.\n",
      "  UserWarning)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('kbest', SelectKBest(k=10, score_func=<function f_classif at 0x000001D758DF30D0>)), ('dec_tree', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best'))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'kbest__k': [5, 10, 20, 40, 60, 100], 'dec_tree__max_depth': [2, 3, 4, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#fit models\n",
    "grid_tree.fit(X_train_zero,y_train_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator:\n",
      "Pipeline(memory=None,\n",
      "     steps=[('kbest', SelectKBest(k=100, score_func=<function f_classif at 0x000001D758DF30D0>)), ('dec_tree', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
      "            splitter='best'))])\n",
      "Decision Tree step:\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
      "            splitter='best')\n",
      "Best Parameters: {'dec_tree__max_depth': 5, 'kbest__k': 100}\n",
      "Best cross validation score: 0.79\n"
     ]
    }
   ],
   "source": [
    "print(\"Best estimator:\\n{}\".format(grid_tree.best_estimator_)) #prints best model and pipeline\n",
    "print(\"Decision Tree step:\\n{}\".format(grid_tree.best_estimator_.named_steps[\"dec_tree\"])) #prints logistic regression step of pipeline\n",
    "#print(\"Logistic Regression coefficients:\\n{}\".format(grid_tree.best_estimator_.named_steps[\"dec_tree\"].coef_)) #prints coefficients of best estimator\n",
    "print(\"Best Parameters: {}\".format(grid_tree.best_params_)) #outputs best parameters settings\n",
    "print(\"Best cross validation score: {:.2f}\".format(grid_tree.best_score_)) #best produced cross validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid scores on Development set: \n",
      "0.681 (+/-0.001) for {'dec_tree__max_depth': 2, 'kbest__k': 5}\n",
      "0.710 (+/-0.047) for {'dec_tree__max_depth': 2, 'kbest__k': 10}\n",
      "0.710 (+/-0.047) for {'dec_tree__max_depth': 2, 'kbest__k': 20}\n",
      "0.710 (+/-0.047) for {'dec_tree__max_depth': 2, 'kbest__k': 40}\n",
      "0.710 (+/-0.047) for {'dec_tree__max_depth': 2, 'kbest__k': 60}\n",
      "0.710 (+/-0.047) for {'dec_tree__max_depth': 2, 'kbest__k': 100}\n",
      "0.713 (+/-0.001) for {'dec_tree__max_depth': 3, 'kbest__k': 5}\n",
      "0.745 (+/-0.009) for {'dec_tree__max_depth': 3, 'kbest__k': 10}\n",
      "0.750 (+/-0.016) for {'dec_tree__max_depth': 3, 'kbest__k': 20}\n",
      "0.751 (+/-0.018) for {'dec_tree__max_depth': 3, 'kbest__k': 40}\n",
      "0.751 (+/-0.018) for {'dec_tree__max_depth': 3, 'kbest__k': 60}\n",
      "0.751 (+/-0.018) for {'dec_tree__max_depth': 3, 'kbest__k': 100}\n",
      "0.747 (+/-0.000) for {'dec_tree__max_depth': 4, 'kbest__k': 5}\n",
      "0.762 (+/-0.013) for {'dec_tree__max_depth': 4, 'kbest__k': 10}\n",
      "0.768 (+/-0.008) for {'dec_tree__max_depth': 4, 'kbest__k': 20}\n",
      "0.771 (+/-0.010) for {'dec_tree__max_depth': 4, 'kbest__k': 40}\n",
      "0.771 (+/-0.010) for {'dec_tree__max_depth': 4, 'kbest__k': 60}\n",
      "0.773 (+/-0.009) for {'dec_tree__max_depth': 4, 'kbest__k': 100}\n",
      "0.772 (+/-0.001) for {'dec_tree__max_depth': 5, 'kbest__k': 5}\n",
      "0.782 (+/-0.014) for {'dec_tree__max_depth': 5, 'kbest__k': 10}\n",
      "0.785 (+/-0.012) for {'dec_tree__max_depth': 5, 'kbest__k': 20}\n",
      "0.789 (+/-0.013) for {'dec_tree__max_depth': 5, 'kbest__k': 40}\n",
      "0.789 (+/-0.013) for {'dec_tree__max_depth': 5, 'kbest__k': 60}\n",
      "0.791 (+/-0.011) for {'dec_tree__max_depth': 5, 'kbest__k': 100}\n",
      "Grid scores on Development set: \n",
      "0.681 (+/-0.004) for {'dec_tree__max_depth': 2, 'kbest__k': 5}\n",
      "0.708 (+/-0.041) for {'dec_tree__max_depth': 2, 'kbest__k': 10}\n",
      "0.708 (+/-0.041) for {'dec_tree__max_depth': 2, 'kbest__k': 20}\n",
      "0.708 (+/-0.041) for {'dec_tree__max_depth': 2, 'kbest__k': 40}\n",
      "0.708 (+/-0.041) for {'dec_tree__max_depth': 2, 'kbest__k': 60}\n",
      "0.708 (+/-0.041) for {'dec_tree__max_depth': 2, 'kbest__k': 100}\n",
      "0.713 (+/-0.003) for {'dec_tree__max_depth': 3, 'kbest__k': 5}\n",
      "0.744 (+/-0.003) for {'dec_tree__max_depth': 3, 'kbest__k': 10}\n",
      "0.750 (+/-0.009) for {'dec_tree__max_depth': 3, 'kbest__k': 20}\n",
      "0.751 (+/-0.011) for {'dec_tree__max_depth': 3, 'kbest__k': 40}\n",
      "0.751 (+/-0.011) for {'dec_tree__max_depth': 3, 'kbest__k': 60}\n",
      "0.751 (+/-0.012) for {'dec_tree__max_depth': 3, 'kbest__k': 100}\n",
      "0.747 (+/-0.002) for {'dec_tree__max_depth': 4, 'kbest__k': 5}\n",
      "0.762 (+/-0.009) for {'dec_tree__max_depth': 4, 'kbest__k': 10}\n",
      "0.767 (+/-0.003) for {'dec_tree__max_depth': 4, 'kbest__k': 20}\n",
      "0.770 (+/-0.004) for {'dec_tree__max_depth': 4, 'kbest__k': 40}\n",
      "0.770 (+/-0.004) for {'dec_tree__max_depth': 4, 'kbest__k': 60}\n",
      "0.772 (+/-0.004) for {'dec_tree__max_depth': 4, 'kbest__k': 100}\n",
      "0.771 (+/-0.002) for {'dec_tree__max_depth': 5, 'kbest__k': 5}\n",
      "0.782 (+/-0.011) for {'dec_tree__max_depth': 5, 'kbest__k': 10}\n",
      "0.785 (+/-0.005) for {'dec_tree__max_depth': 5, 'kbest__k': 20}\n",
      "0.788 (+/-0.008) for {'dec_tree__max_depth': 5, 'kbest__k': 40}\n",
      "0.788 (+/-0.008) for {'dec_tree__max_depth': 5, 'kbest__k': 60}\n",
      "0.790 (+/-0.005) for {'dec_tree__max_depth': 5, 'kbest__k': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "C:\\tooling\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "#AUC scores for each training set run\n",
    "print(\"Grid scores on Development set: \")\n",
    "means = grid_tree.cv_results_['mean_train_score']\n",
    "stds = grid_tree.cv_results_['std_train_score']\n",
    "\n",
    "for mean, std, params in zip(means, stds, grid_tree.cv_results_['params']): #note imbedded grid_l2 will need to be adjusted to correct object\n",
    "    print(\"%0.3f (+/-%0.3f) for %r\"\n",
    "         % (mean, std *2, params))\n",
    "\n",
    "#AUC scores for each validation set run\n",
    "print(\"Grid scores on Development set: \")\n",
    "means = grid_tree.cv_results_['mean_test_score']\n",
    "stds = grid_tree.cv_results_['std_test_score']\n",
    "\n",
    "for mean, std, params in zip(means, stds, grid_tree.cv_results_['params']): #note imbedded grid_l2 will need to be adjusted to correct object\n",
    "    print(\"%0.3f (+/-%0.3f) for %r\"\n",
    "         % (mean, std *2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions of this Notebook:\n",
    "<BR><BR>\n",
    "    **Objectives:**<br>\n",
    "1. Identify optimal hyperparameter structure for models.<br>\n",
    "2. Determine most appropriate missing data imputation method. <br>\n",
    "3. Explore the impact the number of features has on model performance.<br>\n",
    "4. Identify AUC target for first iteration of Risk Model. <br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Optimal parameter structures for each model type has been finalized. For L1 Logistic Regression, the optimal C parameter was 100. For L2 Logistic Regression, the optimal C parameter was 10.<br><br>\n",
    "    \n",
    "2. The most accurate imputation method tested was imputing the columnar median values, rather than setting each NaN to '0'. Moving forward, both methods will be tested on final versions of models, as I'm not currently sure if calculating median values for all missing data is possible in database. It should, but for transparency sake, I should continue testing.<br><br>\n",
    "\n",
    "3. I think I need to do more statistically-based feature reduction, as opposed to automating that process. while a chi-square independence test won't work, ANOVA will and I can use significant p-values to find significance. My only worry is that with so much data, there will be a lot of significant features to build into the model (like 100s). But then again, maybe that is necessary... I'll keep experimenting. I think taking a more statistical approach to feature selection will improve clinical trust of the model, as right now I am explicitly selecting the most informative 20,40,60,etc features to be included in each iteration, but can't say if those features are consistently used across model due to the cross validation and grid search methods. The best approach long-term is to imbed the feature selection process into modeling pipeline, as this is true machine learning/data mining, but I think it would be best to build trust in our process first. <br><br>\n",
    "\n",
    "4. I think I have a decent idea for models and parameters to officially try without the pipeline and grid search methods used above. It seems like I am getting pretty good performance with a limited number of features included in the model, which is great. The decision tree above was able to score an AUC of .73 using a depth of 2 and only 5 features... That's pretty incredible. Additionally, the logistic regressions had an AUC of .75 with only 5 features, which is great. <br><br>\n",
    "\n",
    "Additionally, **I think our target AUC should be around 0.77 for working production model**. kNN, decision tree (even short), and both regressions produced this AUC with little features used. While we can expect a reduction in performance when applied to truly new data, I think an AUC starting around 0.77 is great, with a cap at 0.83 for production-level testing. I think that can be attained relatively easily, and reduces the amount of risk of overtraining features to this dataset, thus reducing the accuracy of the model in production and increases computational processing time. Anything higher than 0.83 for our first testing of this risk model begins to push into the overfitting territory. I'd prefer to be a little more cautious at this stage and then iterate with new model and features, pushing AUC higher.<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NEXT STEPS**: Pursue some statistically-focused feature selection to aide in the interpretation of the model and reduce computational strain. Test final versions of models, again with cross validation, and calculate performance metrics for each version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
